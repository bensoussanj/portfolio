{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSWzy0w5ESWk"
   },
   "source": [
    "# Exercise 1: Building a \"little stemmer\"\n",
    "\n",
    "For this exercise, we will take a sample of Antoine de Saint-Exup√©ry's novella *The Little Prince* and use it to demonstrate tokenization and stemming.\n",
    "\n",
    "Here is your sample text, which appears at the beginning of the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T08:10:46.982640Z",
     "start_time": "2019-01-15T08:10:46.978955Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "38EI6SxlDzZR"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest. It was a picture of a boa constrictor in the act of swallowing an animal. Here is a copy of the drawing.\n",
    "Boa\n",
    "In the book it said: \"Boa constrictors swallow their prey whole, without chewing it. After that they are not able to move, and they sleep through the six months that they need for digestion.\"\n",
    "I pondered deeply, then, over the adventures of the jungle. And after some work with a colored pencil I succeeded in making my first drawing. My Drawing Number One. It looked something like this:\n",
    "Hat\n",
    "I showed my masterpiece to the grown-ups, and asked them whether the drawing frightened them.\n",
    "But they answered: \"Frighten? Why should any one be frightened by a hat?\"\n",
    "My drawing was not a picture of a hat. It was a picture of a boa constrictor digesting an elephant. But since the grown-ups were not able to understand it, I made another drawing: I drew the inside of a boa constrictor, so that the grown-ups could see it clearly. They always need to have things explained. My Drawing Number Two looked like this:\n",
    "Elephant inside the boa\n",
    "The grown-ups' response, this time, was to advise me to lay aside my drawings of boa constrictors, whether from the inside or the outside, and devote myself instead to geography, history, arithmetic, and grammar. That is why, at the age of six, I gave up what might have been a magnificent career as a painter. I had been disheartened by the failure of my Drawing Number One and my Drawing Number Two. Grown-ups never understand anything by themselves, and it is tiresome for children to be always and forever explaining things to them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaxfIOKzFUwy"
   },
   "source": [
    "First let's use NLTK's build-in functions to tokenize and stem this text. First convert the given text into an array of lowercase tokens using the NLTK functions word_tokenize and PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T09:01:26.230939Z",
     "start_time": "2019-01-15T09:01:26.199156Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "duCI57c5B0m1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once:onc\n",
      "when:when\n",
      "I:I\n",
      "was:wa\n",
      "six:six\n",
      "years:year\n",
      "old:old\n",
      "I:I\n",
      "saw:saw\n",
      "a:a\n",
      "magnificent:magnific\n",
      "picture:pictur\n",
      "in:in\n",
      "a:a\n",
      "book:book\n",
      ",:,\n",
      "called:call\n",
      "True:true\n",
      "Stories:stori\n",
      "from:from\n",
      "Nature:natur\n",
      ",:,\n",
      "about:about\n",
      "the:the\n",
      "primeval:primev\n",
      "forest:forest\n",
      ".:.\n",
      "It:It\n",
      "was:wa\n",
      "a:a\n",
      "picture:pictur\n",
      "of:of\n",
      "a:a\n",
      "boa:boa\n",
      "constrictor:constrictor\n",
      "in:in\n",
      "the:the\n",
      "act:act\n",
      "of:of\n",
      "swallowing:swallow\n",
      "an:an\n",
      "animal:anim\n",
      ".:.\n",
      "Here:here\n",
      "is:is\n",
      "a:a\n",
      "copy:copi\n",
      "of:of\n",
      "the:the\n",
      "drawing:draw\n",
      ".:.\n",
      "Boa:boa\n",
      "In:In\n",
      "the:the\n",
      "book:book\n",
      "it:it\n",
      "said:said\n",
      ":::\n",
      "``:``\n",
      "Boa:boa\n",
      "constrictors:constrictor\n",
      "swallow:swallow\n",
      "their:their\n",
      "prey:prey\n",
      "whole:whole\n",
      ",:,\n",
      "without:without\n",
      "chewing:chew\n",
      "it:it\n",
      ".:.\n",
      "After:after\n",
      "that:that\n",
      "they:they\n",
      "are:are\n",
      "not:not\n",
      "able:abl\n",
      "to:to\n",
      "move:move\n",
      ",:,\n",
      "and:and\n",
      "they:they\n",
      "sleep:sleep\n",
      "through:through\n",
      "the:the\n",
      "six:six\n",
      "months:month\n",
      "that:that\n",
      "they:they\n",
      "need:need\n",
      "for:for\n",
      "digestion:digest\n",
      ".:.\n",
      "'':''\n",
      "I:I\n",
      "pondered:ponder\n",
      "deeply:deepli\n",
      ",:,\n",
      "then:then\n",
      ",:,\n",
      "over:over\n",
      "the:the\n",
      "adventures:adventur\n",
      "of:of\n",
      "the:the\n",
      "jungle:jungl\n",
      ".:.\n",
      "And:and\n",
      "after:after\n",
      "some:some\n",
      "work:work\n",
      "with:with\n",
      "a:a\n",
      "colored:color\n",
      "pencil:pencil\n",
      "I:I\n",
      "succeeded:succeed\n",
      "in:in\n",
      "making:make\n",
      "my:my\n",
      "first:first\n",
      "drawing:draw\n",
      ".:.\n",
      "My:My\n",
      "Drawing:draw\n",
      "Number:number\n",
      "One:one\n",
      ".:.\n",
      "It:It\n",
      "looked:look\n",
      "something:someth\n",
      "like:like\n",
      "this:thi\n",
      ":::\n",
      "Hat:hat\n",
      "I:I\n",
      "showed:show\n",
      "my:my\n",
      "masterpiece:masterpiec\n",
      "to:to\n",
      "the:the\n",
      "grown-ups:grown-up\n",
      ",:,\n",
      "and:and\n",
      "asked:ask\n",
      "them:them\n",
      "whether:whether\n",
      "the:the\n",
      "drawing:draw\n",
      "frightened:frighten\n",
      "them:them\n",
      ".:.\n",
      "But:but\n",
      "they:they\n",
      "answered:answer\n",
      ":::\n",
      "``:``\n",
      "Frighten:frighten\n",
      "?:?\n",
      "Why:whi\n",
      "should:should\n",
      "any:ani\n",
      "one:one\n",
      "be:be\n",
      "frightened:frighten\n",
      "by:by\n",
      "a:a\n",
      "hat:hat\n",
      "?:?\n",
      "'':''\n",
      "My:My\n",
      "drawing:draw\n",
      "was:wa\n",
      "not:not\n",
      "a:a\n",
      "picture:pictur\n",
      "of:of\n",
      "a:a\n",
      "hat:hat\n",
      ".:.\n",
      "It:It\n",
      "was:wa\n",
      "a:a\n",
      "picture:pictur\n",
      "of:of\n",
      "a:a\n",
      "boa:boa\n",
      "constrictor:constrictor\n",
      "digesting:digest\n",
      "an:an\n",
      "elephant:eleph\n",
      ".:.\n",
      "But:but\n",
      "since:sinc\n",
      "the:the\n",
      "grown-ups:grown-up\n",
      "were:were\n",
      "not:not\n",
      "able:abl\n",
      "to:to\n",
      "understand:understand\n",
      "it:it\n",
      ",:,\n",
      "I:I\n",
      "made:made\n",
      "another:anoth\n",
      "drawing:draw\n",
      ":::\n",
      "I:I\n",
      "drew:drew\n",
      "the:the\n",
      "inside:insid\n",
      "of:of\n",
      "a:a\n",
      "boa:boa\n",
      "constrictor:constrictor\n",
      ",:,\n",
      "so:so\n",
      "that:that\n",
      "the:the\n",
      "grown-ups:grown-up\n",
      "could:could\n",
      "see:see\n",
      "it:it\n",
      "clearly:clearli\n",
      ".:.\n",
      "They:they\n",
      "always:alway\n",
      "need:need\n",
      "to:to\n",
      "have:have\n",
      "things:thing\n",
      "explained:explain\n",
      ".:.\n",
      "My:My\n",
      "Drawing:draw\n",
      "Number:number\n",
      "Two:two\n",
      "looked:look\n",
      "like:like\n",
      "this:thi\n",
      ":::\n",
      "Elephant:eleph\n",
      "inside:insid\n",
      "the:the\n",
      "boa:boa\n",
      "The:the\n",
      "grown-ups:grown-up\n",
      "':'\n",
      "response:respons\n",
      ",:,\n",
      "this:thi\n",
      "time:time\n",
      ",:,\n",
      "was:wa\n",
      "to:to\n",
      "advise:advis\n",
      "me:me\n",
      "to:to\n",
      "lay:lay\n",
      "aside:asid\n",
      "my:my\n",
      "drawings:draw\n",
      "of:of\n",
      "boa:boa\n",
      "constrictors:constrictor\n",
      ",:,\n",
      "whether:whether\n",
      "from:from\n",
      "the:the\n",
      "inside:insid\n",
      "or:or\n",
      "the:the\n",
      "outside:outsid\n",
      ",:,\n",
      "and:and\n",
      "devote:devot\n",
      "myself:myself\n",
      "instead:instead\n",
      "to:to\n",
      "geography:geographi\n",
      ",:,\n",
      "history:histori\n",
      ",:,\n",
      "arithmetic:arithmet\n",
      ",:,\n",
      "and:and\n",
      "grammar:grammar\n",
      ".:.\n",
      "That:that\n",
      "is:is\n",
      "why:whi\n",
      ",:,\n",
      "at:at\n",
      "the:the\n",
      "age:age\n",
      "of:of\n",
      "six:six\n",
      ",:,\n",
      "I:I\n",
      "gave:gave\n",
      "up:up\n",
      "what:what\n",
      "might:might\n",
      "have:have\n",
      "been:been\n",
      "a:a\n",
      "magnificent:magnific\n",
      "career:career\n",
      "as:as\n",
      "a:a\n",
      "painter:painter\n",
      ".:.\n",
      "I:I\n",
      "had:had\n",
      "been:been\n",
      "disheartened:dishearten\n",
      "by:by\n",
      "the:the\n",
      "failure:failur\n",
      "of:of\n",
      "my:my\n",
      "Drawing:draw\n",
      "Number:number\n",
      "One:one\n",
      "and:and\n",
      "my:my\n",
      "Drawing:draw\n",
      "Number:number\n",
      "Two:two\n",
      ".:.\n",
      "Grown-ups:grown-up\n",
      "never:never\n",
      "understand:understand\n",
      "anything:anyth\n",
      "by:by\n",
      "themselves:themselv\n",
      ",:,\n",
      "and:and\n",
      "it:it\n",
      "is:is\n",
      "tiresome:tiresom\n",
      "for:for\n",
      "children:children\n",
      "to:to\n",
      "be:be\n",
      "always:alway\n",
      "and:and\n",
      "forever:forev\n",
      "explaining:explain\n",
      "things:thing\n",
      "to:to\n",
      "them:them\n",
      ".:.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = word_tokenize(text)\n",
    "stems = set()\n",
    "lowercase_stems = set()\n",
    "\n",
    "for word in words:\n",
    "    stemmed_word = ps.stem(word)\n",
    "    stems.add(stemmed_word)\n",
    "    lowercase_stems.add(stemmed_word.lower())\n",
    "    print(word + \":\" + stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZ4Hh5JpIM7w"
   },
   "source": [
    "**Questions:**\n",
    "  1. How many unique tokens are there in the text?\n",
    "  1. How many unique stemmed tokens are in the text? Lowercase stemmed tokens?\n",
    "  1. What are some examples of words that have surprising stemmed forms? Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T09:01:28.563154Z",
     "start_time": "2019-01-15T09:01:28.555685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 353 unique tokens in the text\n",
      "There are 152 unique stems in the text\n",
      "There are 149 unique lowercase stems in the text\n"
     ]
    }
   ],
   "source": [
    "print('There are {} unique tokens in the text'.format(len(word_tokenize(text))))\n",
    "print('There are {} unique stems in the text'.format(len(stems)))\n",
    "print('There are {} unique lowercase stems in the text'.format(len(lowercase_stems)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words ending with a letter or sequence of letters that can be a suffix to other roots end up looking strange.\n",
    "For example:\n",
    "- 'why' becomes 'whi'\n",
    "-> this is weird but it makes sense for 'geography' that becomes 'geographi' and would be a common stem to the word geographical\n",
    "- 'failure' becomes 'failur'\n",
    "-> other words like 'nature' could have derivates e.g. 'naturally' where a stem could make sense, but in case of 'failure' there are no such derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jec0fStwKy5q"
   },
   "source": [
    "Now let's try writing our own stemmer. Write a function which takes in a token and returns its stem, by removing common English suffixes (e.g. remove the suffix -ed as in *listened* -> *listen*). Try to handle as many suffixes as you can think of. Then use this custom stemmer to convert the given text to an array of lowercase tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T09:30:46.733670Z",
     "start_time": "2019-01-15T09:30:46.715273Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "IUN-s5LuEJdo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ization', 'isation', 'ically', 'izing', 'ation', 'isory', 'ally', 'wise', 'isor', 'ical', 'ing', 'ent', 'ize', 'ary', 'ion', 'ary', 'ize', 'ise', 'ies', 'ate', 'ant', 'ice', 'ome', 'cly', 'ed', 'es', 'er', 'al', 'ly', 'ic', 's', 'e', 'y']\n",
      "Once:Onc\n",
      "when:when\n",
      "I:I\n",
      "was:was\n",
      "six:six\n",
      "years:year\n",
      "old:old\n",
      "I:I\n",
      "saw:saw\n",
      "a:a\n",
      "magnificent:magnific\n",
      "picture:pictur\n",
      "in:in\n",
      "a:a\n",
      "book:book\n",
      ",:,\n",
      "called:call\n",
      "True:Tru\n",
      "Stories:Stor\n",
      "from:from\n",
      "Nature:Natur\n",
      ",:,\n",
      "about:about\n",
      "the:the\n",
      "primeval:primev\n",
      "forest:forest\n",
      ".:.\n",
      "It:It\n",
      "was:was\n",
      "a:a\n",
      "picture:pictur\n",
      "of:of\n",
      "a:a\n",
      "boa:boa\n",
      "constrictor:constrictor\n",
      "in:in\n",
      "the:the\n",
      "act:act\n",
      "of:of\n",
      "swallowing:swallow\n",
      "an:an\n",
      "animal:anim\n",
      ".:.\n",
      "Here:Her\n",
      "is:is\n",
      "a:a\n",
      "copy:cop\n",
      "of:of\n",
      "the:the\n",
      "drawing:draw\n",
      ".:.\n",
      "Boa:Boa\n",
      "In:In\n",
      "the:the\n",
      "book:book\n",
      "it:it\n",
      "said:said\n",
      ":::\n",
      "``:``\n",
      "Boa:Boa\n",
      "constrictors:constrictor\n",
      "swallow:swallow\n",
      "their:their\n",
      "prey:pre\n",
      "whole:whol\n",
      ",:,\n",
      "without:without\n",
      "chewing:chew\n",
      "it:it\n",
      ".:.\n",
      "After:Aft\n",
      "that:that\n",
      "they:the\n",
      "are:are\n",
      "not:not\n",
      "able:abl\n",
      "to:to\n",
      "move:mov\n",
      ",:,\n",
      "and:and\n",
      "they:the\n",
      "sleep:sleep\n",
      "through:through\n",
      "the:the\n",
      "six:six\n",
      "months:month\n",
      "that:that\n",
      "they:the\n",
      "need:ne\n",
      "for:for\n",
      "digestion:digest\n",
      ".:.\n",
      "'':''\n",
      "I:I\n",
      "pondered:ponder\n",
      "deeply:deep\n",
      ",:,\n",
      "then:then\n",
      ",:,\n",
      "over:ov\n",
      "the:the\n",
      "adventures:adventur\n",
      "of:of\n",
      "the:the\n",
      "jungle:jungl\n",
      ".:.\n",
      "And:And\n",
      "after:aft\n",
      "some:s\n",
      "work:work\n",
      "with:with\n",
      "a:a\n",
      "colored:color\n",
      "pencil:pencil\n",
      "I:I\n",
      "succeeded:succeed\n",
      "in:in\n",
      "making:mak\n",
      "my:my\n",
      "first:first\n",
      "drawing:draw\n",
      ".:.\n",
      "My:My\n",
      "Drawing:Draw\n",
      "Number:Numb\n",
      "One:One\n",
      ".:.\n",
      "It:It\n",
      "looked:look\n",
      "something:someth\n",
      "like:lik\n",
      "this:thi\n",
      ":::\n",
      "Hat:Hat\n",
      "I:I\n",
      "showed:show\n",
      "my:my\n",
      "masterpiece:masterpiec\n",
      "to:to\n",
      "the:the\n",
      "grown-ups:grown-up\n",
      ",:,\n",
      "and:and\n",
      "asked:ask\n",
      "them:them\n",
      "whether:wheth\n",
      "the:the\n",
      "drawing:draw\n",
      "frightened:frighten\n",
      "them:them\n",
      ".:.\n",
      "But:But\n",
      "they:the\n",
      "answered:answer\n",
      ":::\n",
      "``:``\n",
      "Frighten:Frighten\n",
      "?:?\n",
      "Why:Why\n",
      "should:should\n",
      "any:any\n",
      "one:one\n",
      "be:be\n",
      "frightened:frighten\n",
      "by:by\n",
      "a:a\n",
      "hat:hat\n",
      "?:?\n",
      "'':''\n",
      "My:My\n",
      "drawing:draw\n",
      "was:was\n",
      "not:not\n",
      "a:a\n",
      "picture:pictur\n",
      "of:of\n",
      "a:a\n",
      "hat:hat\n",
      ".:.\n",
      "It:It\n",
      "was:was\n",
      "a:a\n",
      "picture:pictur\n",
      "of:of\n",
      "a:a\n",
      "boa:boa\n",
      "constrictor:constrictor\n",
      "digesting:digest\n",
      "an:an\n",
      "elephant:eleph\n",
      ".:.\n",
      "But:But\n",
      "since:sinc\n",
      "the:the\n",
      "grown-ups:grown-up\n",
      "were:wer\n",
      "not:not\n",
      "able:abl\n",
      "to:to\n",
      "understand:understand\n",
      "it:it\n",
      ",:,\n",
      "I:I\n",
      "made:mad\n",
      "another:anoth\n",
      "drawing:draw\n",
      ":::\n",
      "I:I\n",
      "drew:drew\n",
      "the:the\n",
      "inside:insid\n",
      "of:of\n",
      "a:a\n",
      "boa:boa\n",
      "constrictor:constrictor\n",
      ",:,\n",
      "so:so\n",
      "that:that\n",
      "the:the\n",
      "grown-ups:grown-up\n",
      "could:could\n",
      "see:see\n",
      "it:it\n",
      "clearly:clear\n",
      ".:.\n",
      "They:The\n",
      "always:alway\n",
      "need:ne\n",
      "to:to\n",
      "have:hav\n",
      "things:thing\n",
      "explained:explain\n",
      ".:.\n",
      "My:My\n",
      "Drawing:Draw\n",
      "Number:Numb\n",
      "Two:Two\n",
      "looked:look\n",
      "like:lik\n",
      "this:thi\n",
      ":::\n",
      "Elephant:Eleph\n",
      "inside:insid\n",
      "the:the\n",
      "boa:boa\n",
      "The:The\n",
      "grown-ups:grown-up\n",
      "':'\n",
      "response:respons\n",
      ",:,\n",
      "this:thi\n",
      "time:tim\n",
      ",:,\n",
      "was:was\n",
      "to:to\n",
      "advise:adv\n",
      "me:me\n",
      "to:to\n",
      "lay:lay\n",
      "aside:asid\n",
      "my:my\n",
      "drawings:drawing\n",
      "of:of\n",
      "boa:boa\n",
      "constrictors:constrictor\n",
      ",:,\n",
      "whether:wheth\n",
      "from:from\n",
      "the:the\n",
      "inside:insid\n",
      "or:or\n",
      "the:the\n",
      "outside:outsid\n",
      ",:,\n",
      "and:and\n",
      "devote:devot\n",
      "myself:myself\n",
      "instead:instead\n",
      "to:to\n",
      "geography:geograph\n",
      ",:,\n",
      "history:histor\n",
      ",:,\n",
      "arithmetic:arithmet\n",
      ",:,\n",
      "and:and\n",
      "grammar:grammar\n",
      ".:.\n",
      "That:That\n",
      "is:is\n",
      "why:why\n",
      ",:,\n",
      "at:at\n",
      "the:the\n",
      "age:age\n",
      "of:of\n",
      "six:six\n",
      ",:,\n",
      "I:I\n",
      "gave:gav\n",
      "up:up\n",
      "what:what\n",
      "might:might\n",
      "have:hav\n",
      "been:been\n",
      "a:a\n",
      "magnificent:magnific\n",
      "career:care\n",
      "as:as\n",
      "a:a\n",
      "painter:paint\n",
      ".:.\n",
      "I:I\n",
      "had:had\n",
      "been:been\n",
      "disheartened:dishearten\n",
      "by:by\n",
      "the:the\n",
      "failure:failur\n",
      "of:of\n",
      "my:my\n",
      "Drawing:Draw\n",
      "Number:Numb\n",
      "One:One\n",
      "and:and\n",
      "my:my\n",
      "Drawing:Draw\n",
      "Number:Numb\n",
      "Two:Two\n",
      ".:.\n",
      "Grown-ups:Grown-up\n",
      "never:nev\n",
      "understand:understand\n",
      "anything:anyth\n",
      "by:by\n",
      "themselves:themselv\n",
      ",:,\n",
      "and:and\n",
      "it:it\n",
      "is:is\n",
      "tiresome:tires\n",
      "for:for\n",
      "children:children\n",
      "to:to\n",
      "be:be\n",
      "always:alway\n",
      "and:and\n",
      "forever:forev\n",
      "explaining:explain\n",
      "things:thing\n",
      "to:to\n",
      "them:them\n",
      ".:.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "suffixes = ['s', 'ed', 'ing', 'e', 'es', 'y', 'ent', 'er', 'ize', 'ary', 'al', 'ly', 'ally', 'ion', 'ary', 'ic', 'izing', 'wise',\n",
    "           'ize', 'ization', 'isation', 'ise', 'ies', 'ate', 'ation', 'ant', 'ice', 'isor', 'isory', 'ome', 'ically', 'cly', 'ical']\n",
    "\n",
    "# sort the suffixes by decreasing length so that the most letters we find the better\n",
    "def sort_suffixes():\n",
    "    global suffixes\n",
    "    suffixes.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "def jeremy_stemmer(word):\n",
    "    # don't stem 3 letter words\n",
    "    if len(word) <= 3:\n",
    "        return word\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    \n",
    "    # no change\n",
    "    return word\n",
    "    \n",
    "sort_suffixes()\n",
    "print(suffixes)\n",
    "\n",
    "for word in words:\n",
    "    print(word + \":\" + jeremy_stemmer(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXjxQgsoK8EM"
   },
   "source": [
    "**Questions:**\n",
    "  4. What are some examples where  your stemmer on the text differs from the PorterStemmer?\n",
    "  5. Can you explain why the differences occur?\n",
    "  \n",
    "**Bonus**: Use NLTK's WordNetLemmatizer to get an array of lemmatized tokens. Where does it differ from the stemmers' outputs? Why?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eNguN9pNMei"
   },
   "source": [
    "I have decided to only remove suffixes, where the PorterStemmer seems to be more sophisticated: it is converting the 'y' suffix into 'i'.\n",
    "I don't really know in which case this would be relevant though.\n",
    "Also, I have added some logic:\n",
    "- 3 letters words or less are ignored in my stemmer, while the PorterStemmer seems again more sophisticated: the word 'the' stays the same, but the word 'was' becomes 'wa' so only some words seem to be ignored.\n",
    "- sort the array of suffixes by descending suffix length, that way we make sure a word like 'alphabetization' becomes 'alphabet' and not 'alphabetizat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T09:33:39.766116Z",
     "start_time": "2019-01-15T09:33:39.739010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeremybensoussan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T09:42:22.477793Z",
     "start_time": "2019-01-15T09:42:22.465780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'digest', 'year', 'constrictor', 'wa', 'look', 'ponder', 'show', 'frighten', 'draw', 'month', 'dishearten', 'color', 'chew', 'give', 'a', 'call', 'have', 'child', 'swallow', 'answer', 'thing', 'succeed', 'be', 'adventure', 'make', 'ask', 'say', 'explain', 'drawing'}\n"
     ]
    }
   ],
   "source": [
    "wl = WordNetLemmatizer()\n",
    "lemmatized_tokens = set()\n",
    "# lemmatized_tokens = {wl.lemmatize(word) for word in words}\n",
    "for word in words:\n",
    "    for part_of_speech in ['a', 's', 'r', 'n', 'v']:\n",
    "        lemma = wl.lemmatize(word, part_of_speech)\n",
    "        if lemma != word:\n",
    "            lemmatized_tokens.add(lemma)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the different parts of speech that the lemmatize() function knows, we were able to transform words like 'was' into 'be', which the stemmer doesn't know to do."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Core 2 - Exercise 1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
