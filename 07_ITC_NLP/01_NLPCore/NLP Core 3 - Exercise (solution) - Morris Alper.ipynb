{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP Core 3 - Exercise (solution).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"JxXPmYWLroVT","colab_type":"text"},"cell_type":"markdown","source":["# NLP Core 3 Exercise: Bagged News\n","\n","In this exercise we will learn how to perform document classification in order to predict the category of news articles from the Reuters Corpus using a **bag-of-words** model and **one-hot encoding**. We will then see how we can use **TF-IDF** to improve our features for classification.\n","\n","## The Reuters Corpus\n","\n","The Reuters Corpus is a collection of news documents along with category tags that are commonly used to test document classification. It is split into two sets: the *training* documents used to train a classification algorithm, and the *test* documents used to test the classifier's performance. Here we load the corpus and save the IDs of the training and test documents:"]},{"metadata":{"id":"L0Xkao6YndgF","colab_type":"code","outputId":"0c05d98f-099d-485d-fc26-dc49ba26507e","executionInfo":{"status":"ok","timestamp":1547516216892,"user_tz":-120,"elapsed":13584,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["import nltk\n","nltk.download('reuters')\n","from nltk.corpus import reuters\n","train_ids = [d for d in reuters.fileids() if d.startswith('train')]\n","test_ids = [d for d in reuters.fileids() if d.startswith('test')]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package reuters to /root/nltk_data...\n","[nltk_data]   Package reuters is already up-to-date!\n"],"name":"stdout"}]},{"metadata":{"id":"-Z5InUoyznqQ","colab_type":"text"},"cell_type":"markdown","source":["**Questions**:\n","  1. How many documents are in the Reuters Corpus? What percentage are training and what percentage are testing documents?\n","  2. How many words are in the training documents? (use *reuters.words(file_id)*)"]},{"metadata":{"id":"m7pS6EGI19hX","colab_type":"code","outputId":"d9529f17-48b6-4957-da0d-13365b04e28b","executionInfo":{"status":"ok","timestamp":1547516219879,"user_tz":-120,"elapsed":16558,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["# answer to 1\n","n_total = len(train_ids + test_ids)\n","percent_training = round(len(train_ids) / n_total * 100)\n","print(len(train_ids + test_ids), 'documents in Reuters')\n","print(f'{percent_training}% training, {100 - percent_training}% testing')\n","# answer to 2\n","print(sum(len(reuters.words(train_id)) for train_id in train_ids), 'words in training documents')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10788 documents in Reuters\n","72% training, 28% testing\n","1253696 words in training documents\n"],"name":"stdout"}]},{"metadata":{"id":"W_mBSLmf1-CM","colab_type":"text"},"cell_type":"markdown","source":["Let's have a look at the categories in the Reuters Corpus. Note that one document can have more than one category:"]},{"metadata":{"id":"fX3kYVeR0zJT","colab_type":"code","outputId":"529f025c-e4d5-4cbd-ba44-b4f69f611194","executionInfo":{"status":"ok","timestamp":1547516219882,"user_tz":-120,"elapsed":16547,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["print(len(reuters.categories()), 'categories in the Reuters Corpus')\n","print('Categories of one sample document:', reuters.categories(train_ids[9]))\n","print('Sample from that document:', reuters.raw(train_ids[9])[:98])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["90 categories in the Reuters Corpus\n","Categories of one sample document: ['cocoa', 'coffee', 'sugar']\n","Sample from that document: COFFEE, SUGAR AND COCOA EXCHANGE NAMES CHAIRMAN\n","  The New York Coffee, Sugar and Cocoa\n","  Exchange \n"],"name":"stdout"}]},{"metadata":{"id":"3i1T0vGV00SR","colab_type":"text"},"cell_type":"markdown","source":["**Question:**\n","  3. What are the three most common categories in the training documents? (use *reuters.categories(file_id)*)"]},{"metadata":{"id":"b3xBRxDWmr_F","colab_type":"code","outputId":"d329d48c-9b78-448b-da73-aa4943add538","executionInfo":{"status":"ok","timestamp":1547516219885,"user_tz":-120,"elapsed":16540,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#answer to 3\n","from nltk import FreqDist\n","fd = FreqDist([category for train_id in train_ids for category in reuters.categories(train_id)])\n","print('most common 3 categories:', fd.most_common(3))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["most common 3 categories: [('earn', 2877), ('acq', 1650), ('money-fx', 538)]\n"],"name":"stdout"}]},{"metadata":{"id":"MyuVHjW04jn4","colab_type":"text"},"cell_type":"markdown","source":["## Bag of words representations\n","\n","We will now see how a sentence can be transformed into a feature vector using a bag of words model. Consider the following sentences:"]},{"metadata":{"id":"Ntr9TapW-Rfl","colab_type":"code","colab":{}},"cell_type":"code","source":["sentences = [\n","  'This is the first document.',\n","  'This document is the second document.',\n","  'And this is the third one.',\n","   'Is this the first document?',\n","]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7ymyaIGW-VSC","colab_type":"text"},"cell_type":"markdown","source":["We can represent each word as a **one-hot** encoded vector (with a single 1 in the column for that word), and add their vectors together to get the feature vector for a sentence:"]},{"metadata":{"id":"6YKVYB8kuwHT","colab_type":"code","outputId":"588c0fcd-3807-4224-c9fd-d45edf0f2034","executionInfo":{"status":"ok","timestamp":1547516219894,"user_tz":-120,"elapsed":16527,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(sentences)\n","X.toarray()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n","       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n","       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n","       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"id":"RiVWmYsO_xRi","colab_type":"text"},"cell_type":"markdown","source":["**Questions:**\n","  4. What do the rows and columns of the feature matrix X represent? **rows: sentences; columns: words**\n","  5. What word does the second column of X represent? What about the third column? (If you are stuck, look at *vectorizer.get_feature_names()*) **document; first**\n"," \n"," **Bonus**: Try using TfidfVectorizer instead of CountVectorizer, and try to explain why some values of X become smaller than others."]},{"metadata":{"id":"rtbsBaVu7Yew","colab_type":"code","outputId":"afc25293-d31e-421f-f911-3e3ffbdff39d","executionInfo":{"status":"ok","timestamp":1547516219896,"user_tz":-120,"elapsed":16519,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["# answer for bonus question\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","X2 = vectorizer.fit_transform(sentences)\n","X2.toarray()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n","        0.        , 0.38408524, 0.        , 0.38408524],\n","       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n","        0.53864762, 0.28108867, 0.        , 0.28108867],\n","       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n","        0.        , 0.26710379, 0.51184851, 0.26710379],\n","       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n","        0.        , 0.38408524, 0.        , 0.38408524]])"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"WA8pQjnp7I96","colab_type":"text"},"cell_type":"markdown","source":["## Classifying Reuters\n","\n","Now let's put these together in order to build a classifier for Reuters articles. Fill in the following code using the instructions in the questions below:"]},{"metadata":{"id":"TgICi3KeH-kD","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import classification_report\n","\n","train_docs = [reuters.raw(train_id) for train_id in train_ids]\n","test_docs = [reuters.raw(test_id) for test_id in test_ids]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t1bNOLIi7KTK","colab_type":"code","colab":{}},"cell_type":"code","source":["#### (A) add code here from question 6\n","# answer:\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(train_docs)\n","X2 = vectorizer.transform(test_docs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MGFbm4Q2H9NK","colab_type":"code","colab":{}},"cell_type":"code","source":["# convert the category labels into binary features for classification\n","mlb = MultiLabelBinarizer()\n","y = mlb.fit_transform([reuters.categories(train_id) for train_id in train_ids])\n","y2 = mlb.transform([reuters.categories(test_id) for test_id in test_ids])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Smn4CymMH9iB","colab_type":"code","outputId":"50040e29-2a6c-4e98-8e3c-fc79ecebd8e8","executionInfo":{"status":"ok","timestamp":1547516233586,"user_tz":-120,"elapsed":30168,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"cell_type":"code","source":["#### (B) add code here from question 7\n","# answer:\n","classifier = OneVsRestClassifier(LinearSVC())\n","classifier.fit(X, y)\n","predictions = classifier.predict(X2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"fvVS7qp3IXBj","colab_type":"code","outputId":"fb6579b6-ce59-4144-b3dd-4cd62171a599","executionInfo":{"status":"ok","timestamp":1547516233590,"user_tz":-120,"elapsed":30159,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":1771}},"cell_type":"code","source":["# show classifier's performance (look at average scores at the bottom)\n","print(classification_report(y2, predictions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.97      0.95      0.96       719\n","           1       1.00      0.39      0.56        23\n","           2       1.00      0.64      0.78        14\n","           3       0.78      0.70      0.74        30\n","           4       0.92      0.67      0.77        18\n","           5       0.00      0.00      0.00         1\n","           6       1.00      0.83      0.91        18\n","           7       0.00      0.00      0.00         2\n","           8       0.00      0.00      0.00         3\n","           9       0.93      0.93      0.93        28\n","          10       1.00      0.78      0.88        18\n","          11       0.00      0.00      0.00         1\n","          12       0.91      0.86      0.88        56\n","          13       1.00      0.50      0.67        20\n","          14       0.00      0.00      0.00         2\n","          15       0.68      0.46      0.55        28\n","          16       0.00      0.00      0.00         1\n","          17       0.84      0.85      0.85       189\n","          18       0.00      0.00      0.00         1\n","          19       0.78      0.82      0.80        44\n","          20       0.00      0.00      0.00         4\n","          21       0.97      0.98      0.98      1087\n","          22       0.67      0.20      0.31        10\n","          23       1.00      0.53      0.69        17\n","          24       0.90      0.80      0.85        35\n","          25       0.92      0.73      0.81        30\n","          26       0.90      0.81      0.86       149\n","          27       0.00      0.00      0.00         4\n","          28       0.00      0.00      0.00         1\n","          29       0.50      0.60      0.55         5\n","          30       1.00      0.33      0.50         6\n","          31       1.00      0.75      0.86         4\n","          32       1.00      0.29      0.44         7\n","          33       0.00      0.00      0.00         1\n","          34       0.80      0.69      0.74       131\n","          35       0.75      1.00      0.86        12\n","          36       0.70      0.50      0.58        14\n","          37       0.00      0.00      0.00         1\n","          38       0.71      0.57      0.63        21\n","          39       0.00      0.00      0.00         2\n","          40       1.00      0.57      0.73        14\n","          41       1.00      1.00      1.00         3\n","          42       0.00      0.00      0.00         1\n","          43       0.56      0.42      0.48        24\n","          44       0.00      0.00      0.00         6\n","          45       0.80      0.21      0.33        19\n","          46       0.74      0.72      0.73       179\n","          47       0.72      0.76      0.74        34\n","          48       0.00      0.00      0.00         4\n","          49       0.77      0.57      0.65        30\n","          50       0.00      0.00      0.00         1\n","          51       0.00      0.00      0.00         2\n","          52       0.00      0.00      0.00         2\n","          53       0.17      0.33      0.22         6\n","          54       0.69      0.62      0.65        47\n","          55       1.00      0.64      0.78        11\n","          56       0.00      0.00      0.00         1\n","          57       1.00      0.50      0.67        10\n","          58       0.00      0.00      0.00         1\n","          59       0.00      0.00      0.00        12\n","          60       1.00      0.14      0.25         7\n","          61       1.00      0.33      0.50         3\n","          62       0.00      0.00      0.00         3\n","          63       0.00      0.00      0.00         1\n","          64       0.00      0.00      0.00         3\n","          65       1.00      0.44      0.62         9\n","          66       0.86      0.67      0.75        18\n","          67       1.00      0.50      0.67         2\n","          68       1.00      0.46      0.63        24\n","          69       1.00      0.67      0.80        12\n","          70       0.00      0.00      0.00         1\n","          71       0.77      0.69      0.73        89\n","          72       1.00      0.50      0.67         8\n","          73       0.67      0.20      0.31        10\n","          74       1.00      0.15      0.27        13\n","          75       0.33      0.09      0.14        11\n","          76       0.80      0.61      0.69        33\n","          77       0.00      0.00      0.00        11\n","          78       0.93      0.78      0.85        36\n","          79       0.00      0.00      0.00         1\n","          80       0.00      0.00      0.00         2\n","          81       0.00      0.00      0.00         5\n","          82       0.00      0.00      0.00         4\n","          83       1.00      0.58      0.74        12\n","          84       0.70      0.71      0.70       117\n","          85       0.86      0.51      0.64        37\n","          86       0.89      0.82      0.85        71\n","          87       0.90      0.90      0.90        10\n","          88       0.33      0.21      0.26        14\n","          89       1.00      0.38      0.56        13\n","\n","   micro avg       0.90      0.81      0.85      3744\n","   macro avg       0.56      0.39      0.44      3744\n","weighted avg       0.88      0.81      0.83      3744\n"," samples avg       0.86      0.86      0.85      3744\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"}]},{"metadata":{"id":"jbJDXtPdHa6k","colab_type":"text"},"cell_type":"markdown","source":["**Questions:**\n","  6. In (A) above, add code to convert the training and testing documents into matrices X and X2 of feature vectors using CountVectorizer(). (Hint: use fit_transform() first on the training set, and then transform() on the testing set)\n","  7. In (B) above, add code to fit a multiclass SVM classifier on the training data . (Hint: use *OneVsRestClassifier(LinearSVC())* as the classifier object, and then call its fit() and predict() methods on the data.)\n","  \n"," **Bonus**: Try using TF-IDF (TfidfVectorizer) weighted features. Does the classifier's performance improve?"]},{"metadata":{"id":"zqyl4nr3DkmW","colab_type":"code","outputId":"74aadf27-03b3-4d10-db3f-01f4aeea4252","executionInfo":{"status":"ok","timestamp":1547516238465,"user_tz":-120,"elapsed":35020,"user":{"displayName":"Morris Alper","photoUrl":"https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg","userId":"15842932163458061285"}},"colab":{"base_uri":"https://localhost:8080/","height":1771}},"cell_type":"code","source":["# answer to bonus question:\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(train_docs)\n","X2 = vectorizer.transform(test_docs)\n","classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n","classifier.fit(X, y)\n","predictions = classifier.predict(X2)\n","print(classification_report(y2, predictions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.98      0.95      0.97       719\n","           1       1.00      0.39      0.56        23\n","           2       1.00      0.64      0.78        14\n","           3       0.94      0.53      0.68        30\n","           4       0.88      0.39      0.54        18\n","           5       0.00      0.00      0.00         1\n","           6       1.00      0.94      0.97        18\n","           7       0.00      0.00      0.00         2\n","           8       0.00      0.00      0.00         3\n","           9       0.96      0.96      0.96        28\n","          10       1.00      0.78      0.88        18\n","          11       0.00      0.00      0.00         1\n","          12       0.95      0.75      0.84        56\n","          13       1.00      0.50      0.67        20\n","          14       0.00      0.00      0.00         2\n","          15       0.92      0.39      0.55        28\n","          16       0.00      0.00      0.00         1\n","          17       0.90      0.87      0.88       189\n","          18       0.00      0.00      0.00         1\n","          19       0.83      0.68      0.75        44\n","          20       0.00      0.00      0.00         4\n","          21       0.99      0.98      0.98      1087\n","          22       1.00      0.20      0.33        10\n","          23       1.00      0.53      0.69        17\n","          24       1.00      0.74      0.85        35\n","          25       0.91      0.70      0.79        30\n","          26       0.98      0.83      0.89       149\n","          27       0.00      0.00      0.00         4\n","          28       0.00      0.00      0.00         1\n","          29       1.00      0.60      0.75         5\n","          30       1.00      0.33      0.50         6\n","          31       1.00      0.50      0.67         4\n","          32       1.00      0.29      0.44         7\n","          33       0.00      0.00      0.00         1\n","          34       0.87      0.67      0.76       131\n","          35       1.00      0.92      0.96        12\n","          36       0.77      0.71      0.74        14\n","          37       0.00      0.00      0.00         1\n","          38       1.00      0.57      0.73        21\n","          39       0.00      0.00      0.00         2\n","          40       1.00      0.07      0.13        14\n","          41       1.00      1.00      1.00         3\n","          42       0.00      0.00      0.00         1\n","          43       0.73      0.33      0.46        24\n","          44       0.00      0.00      0.00         6\n","          45       1.00      0.11      0.19        19\n","          46       0.80      0.77      0.78       179\n","          47       0.87      0.76      0.81        34\n","          48       0.00      0.00      0.00         4\n","          49       0.79      0.50      0.61        30\n","          50       0.00      0.00      0.00         1\n","          51       0.00      0.00      0.00         2\n","          52       0.00      0.00      0.00         2\n","          53       1.00      0.17      0.29         6\n","          54       0.81      0.47      0.59        47\n","          55       1.00      0.64      0.78        11\n","          56       0.00      0.00      0.00         1\n","          57       1.00      0.50      0.67        10\n","          58       0.00      0.00      0.00         1\n","          59       0.00      0.00      0.00        12\n","          60       0.00      0.00      0.00         7\n","          61       1.00      0.33      0.50         3\n","          62       0.00      0.00      0.00         3\n","          63       0.00      0.00      0.00         1\n","          64       0.00      0.00      0.00         3\n","          65       1.00      0.44      0.62         9\n","          66       0.91      0.56      0.69        18\n","          67       1.00      0.50      0.67         2\n","          68       0.89      0.33      0.48        24\n","          69       1.00      0.67      0.80        12\n","          70       0.00      0.00      0.00         1\n","          71       0.90      0.69      0.78        89\n","          72       1.00      0.12      0.22         8\n","          73       0.75      0.30      0.43        10\n","          74       1.00      0.15      0.27        13\n","          75       0.00      0.00      0.00        11\n","          76       0.83      0.45      0.59        33\n","          77       0.00      0.00      0.00        11\n","          78       0.96      0.75      0.84        36\n","          79       0.00      0.00      0.00         1\n","          80       0.00      0.00      0.00         2\n","          81       1.00      0.20      0.33         5\n","          82       0.00      0.00      0.00         4\n","          83       1.00      0.58      0.74        12\n","          84       0.84      0.71      0.77       117\n","          85       0.94      0.46      0.62        37\n","          86       0.93      0.77      0.85        71\n","          87       1.00      0.60      0.75        10\n","          88       0.00      0.00      0.00        14\n","          89       1.00      0.46      0.63        13\n","\n","   micro avg       0.95      0.79      0.86      3744\n","   macro avg       0.60      0.35      0.42      3744\n","weighted avg       0.92      0.79      0.83      3744\n"," samples avg       0.88      0.86      0.86      3744\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"}]}]}