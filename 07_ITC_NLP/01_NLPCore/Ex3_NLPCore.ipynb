{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxXPmYWLroVT"
   },
   "source": [
    "# NLP Core 3 Exercise: Bagged News\n",
    "\n",
    "In this exercise we will learn how to perform document classification in order to predict the category of news articles from the Reuters Corpus using a **bag-of-words** model and **one-hot encoding**. We will then see how we can use **TF-IDF** to improve our features for classification.\n",
    "\n",
    "## The Reuters Corpus\n",
    "\n",
    "The Reuters Corpus is a collection of news documents along with category tags that are commonly used to test document classification. It is split into two sets: the *training* documents used to train a classification algorithm, and the *test* documents used to test the classifier's performance. Here we load the corpus and save the IDs of the training and test documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13584,
     "status": "ok",
     "timestamp": 1547516216892,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg",
      "userId": "15842932163458061285"
     },
     "user_tz": -120
    },
    "id": "L0Xkao6YndgF",
    "outputId": "0c05d98f-099d-485d-fc26-dc49ba26507e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "train_ids = [d for d in reuters.fileids() if d.startswith('train')]\n",
    "test_ids = [d for d in reuters.fileids() if d.startswith('test')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Z5InUoyznqQ"
   },
   "source": [
    "**Questions**:\n",
    "  1. How many documents are in the Reuters Corpus? What percentage are training and what percentage are testing documents?\n",
    "  2. How many words are in the training documents? (use *reuters.words(file_id)*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_mBSLmf1-CM"
   },
   "source": [
    "Let's have a look at the categories in the Reuters Corpus. Note that one document can have more than one category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16547,
     "status": "ok",
     "timestamp": 1547516219882,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg",
      "userId": "15842932163458061285"
     },
     "user_tz": -120
    },
    "id": "fX3kYVeR0zJT",
    "outputId": "529f025c-e4d5-4cbd-ba44-b4f69f611194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 categories in the Reuters Corpus\n",
      "Categories of one sample document: ['cocoa', 'coffee', 'sugar']\n",
      "Sample from that document: COFFEE, SUGAR AND COCOA EXCHANGE NAMES CHAIRMAN\n",
      "  The New York Coffee, Sugar and Cocoa\n",
      "  Exchange \n"
     ]
    }
   ],
   "source": [
    "print(len(reuters.categories()), 'categories in the Reuters Corpus')\n",
    "print('Categories of one sample document:', reuters.categories(train_ids[9]))\n",
    "print('Sample from that document:', reuters.raw(train_ids[9])[:98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3i1T0vGV00SR"
   },
   "source": [
    "**Question:**\n",
    "  3. What are the three most common categories in the training documents? (use *reuters.categories(file_id)*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyuVHjW04jn4"
   },
   "source": [
    "## Bag of words representations\n",
    "\n",
    "We will now see how a sentence can be transformed into a feature vector using a bag of words model. Consider the following sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ntr9TapW-Rfl"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'This is the first document.',\n",
    "  'This document is the second document.',\n",
    "  'And this is the third one.',\n",
    "   'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ymyaIGW-VSC"
   },
   "source": [
    "We can represent each word as a **one-hot** encoded vector (with a single 1 in the column for that word), and add their vectors together to get the feature vector for a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16527,
     "status": "ok",
     "timestamp": 1547516219894,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg",
      "userId": "15842932163458061285"
     },
     "user_tz": -120
    },
    "id": "6YKVYB8kuwHT",
    "outputId": "588c0fcd-3807-4224-c9fd-d45edf0f2034"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RiVWmYsO_xRi"
   },
   "source": [
    "**Questions:**\n",
    "  4. What do the rows and columns of the feature matrix X represent?\n",
    "  5. What word does the second column of X represent? What about the third column? (If you are stuck, look at *vectorizer.get_feature_names()*)\n",
    " \n",
    " **Bonus**: Try using TfidfVectorizer instead of CountVectorizer, and try to explain why some values of X become smaller than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA8pQjnp7I96"
   },
   "source": [
    "## Classifying Reuters\n",
    "\n",
    "Now let's put these together in order to build a classifier for Reuters articles. Fill in the following code using the instructions in the questions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TgICi3KeH-kD"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_docs = [reuters.raw(train_id) for train_id in train_ids]\n",
    "test_docs = [reuters.raw(test_id) for test_id in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1bNOLIi7KTK"
   },
   "outputs": [],
   "source": [
    "#### (A) add code here from question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGFbm4Q2H9NK"
   },
   "outputs": [],
   "source": [
    "# convert the category labels into binary features for classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform([reuters.categories(train_id) for train_id in train_ids])\n",
    "y2 = mlb.transform([reuters.categories(test_id) for test_id in test_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Smn4CymMH9iB"
   },
   "outputs": [],
   "source": [
    "#### (B) add code here from question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvVS7qp3IXBj"
   },
   "outputs": [],
   "source": [
    "# show classifier's performance (look at average scores at the bottom)\n",
    "print(classification_report(y2, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbJDXtPdHa6k"
   },
   "source": [
    "**Questions:**\n",
    "  6. In (A) above, add code to convert the training and testing documents into matrices X and X2 of feature vectors using CountVectorizer(). (Hint: use fit_transform() first on the training set, and then transform() on the testing set)\n",
    "  7. In (B) above, add code to fit a multiclass SVM classifier on the training data . (Hint: use *OneVsRestClassifier(LinearSVC())* as the classifier object, and then call its fit() and predict() methods on the data.)\n",
    "  \n",
    " **Bonus**: Try using TF-IDF (TfidfVectorizer) weighted features. Does the classifier's performance improve?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Core 3 - Exercise.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
