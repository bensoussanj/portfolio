{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex2_NMT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "CKQdeaqDVUDq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# seq2seq Workshop Excercise 2: Transliteration\n",
        "\n",
        "In this excercise we will train a seq2seq model to transliterate Hebrew text into Latin characters, without any prior knowledge of Hebrew.\n",
        "\n",
        "## Part 1: Hebrew Unicode\n",
        "\n",
        "For our purposes it will be useful to know a bit about how text in Hebrew is encoded in Python strings.\n",
        "\n",
        "Recall that in Python a string is made up of **characters** than can be accessed with square brackets. The length of the string is the number of characters it contains:\n"
      ]
    },
    {
      "metadata": {
        "id": "SAC8qjCQl3OI",
        "colab_type": "code",
        "outputId": "75c8d395-3e86-4727-91a5-155bda735a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"hello\"[1], \"hello\"[4], len(\"hello\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e o 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WK7rA8V6l5SE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In Python 3, a string is a sequence of **Unicode code points**, or unique numeric identifiers for each character. Python lets us see the Unicode code point for a character by using the built-in function *ord*:"
      ]
    },
    {
      "metadata": {
        "id": "_ybIlFDVnc1P",
        "colab_type": "code",
        "outputId": "90f1a311-8cd9-406c-eb3a-78a0a0949d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Unicode code points for characters in 'hello':\", *[ord(char) for char in \"hello\"])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unicode code points for characters in 'hello': 104 101 108 108 111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UB3rxxdjnowo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions**\n",
        "  1. What are the Unicode code points for each character in the word \"naivete\"? What about when it is written \"naïveté\"?\n",
        "  2. Use the built-in Python function *hex* to get the hexidecimal (base-16) values for these code points. What are they?\n",
        "  3. Use the [Show Unicode Character](http://qaz.wtf/u/show.cgi) tool to look at the Unicode characters in each of these two words. Where can we see the code point values? What about the names of the unicode characters?\n",
        "  4. What is the difference between the words \"naïveté\" and \"naïveté\"? What is the length of each as a Python string?"
      ]
    },
    {
      "metadata": {
        "id": "cXXjYGmYjYo_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9d08ef68-6349-4981-b42d-1b6a15c1ed9d"
      },
      "cell_type": "code",
      "source": [
        "# 1\n",
        "print(\"Unicode code points for characters in 'naivete':\", *[ord(char) for char in \"naivete\"])\n",
        "print(\"Unicode code points for characters in 'naïveté':\", *[ord(char) for char in \"naïveté\"])\n",
        "# The encoding for i and ï or e and é are completely different in the unicode mapping table\n",
        "\n",
        "# 2\n",
        "print(\"\\nHex values of Unicode characters in 'naïveté':\", *[hex(ord(char)) for char in \"naïveté\"])\n",
        "\n",
        "# 3\n",
        "# The Show Unicode Character displays the Unicode value of each character, followed by the hex value, the character itself and a plain English description\n",
        "\n",
        "# 4\n",
        "print(\"\\nAlthough they look identical, len('naïveté')={} and len('naïveté')={}.\".format(len('naïveté'), len('naïveté')))\n",
        "print(\"This is not magic, but one is using the combining character feature of Unicode where the other uses one character that renders identically\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unicode code points for characters in 'naivete': 110 97 105 118 101 116 101\n",
            "Unicode code points for characters in 'naïveté': 110 97 239 118 101 116 233\n",
            "\n",
            "Hex values of Unicode characters in 'naïveté': 0x6e 0x61 0xef 0x76 0x65 0x74 0xe9\n",
            "\n",
            "Although they look identical, len('naïveté')=7 and len('naïveté')=9.\n",
            "This is not magic, but one is using the combining character feature of Unicode where the other uses one character that renders identically\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WVMF2oPLpyS3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hebrew words can be written either without vowels, or with vowel symbols called **nikkud**. Let's consider how these are represented in Python and in Unicode.\n",
        "\n",
        "**Questions:**\n",
        "  5. What are the first and last letters in the Python string for the Hebrew word בלשנות? What are their hexidecimal Unicode codepoints?\n",
        "  6. How many characters does the Hebrew string בַּלְשָׁנוּת have? Why this number?\n",
        "  7. What are the second and third characters of יִשְׂרָאֵל? What are their hexidecimal Unicode codepoints?\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "IOh85TGCovWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "39b5b812-5923-4876-c3ba-b39b664b0e75"
      },
      "cell_type": "code",
      "source": [
        "# 5\n",
        "print(\"First and last letters of בלשנות are respectively: {} and {}\".format('בלשנות'[0],'בלשנות'[-1]))\n",
        "print(\"This proves us that right-to-left is only a display feature but in memory the characters are in the correct order\")\n",
        "\n",
        "# 6\n",
        "print(\"\\nThe word 'בַּלְשָׁנוּת' has a length equal to {}. This is because each nikkud is a combining character to its letter.\".format(len('בַּלְשָׁנוּת')))\n",
        "print(\"In the word 'בַּלְשָׁנוּת', we can count 6 nikkudim in addition to the 6 letters of the word giving a count of 12.\")\n",
        "\n",
        "# 7\n",
        "print(\"\\nThe second and third characters of 'יִשְׂרָאֵל' are respectively: {} and {}, with hex values: {} and {}.\".format('יִשְׂרָאֵל'[1], 'יִשְׂרָאֵל'[2], hex(ord('יִשְׂרָאֵל'[1])), hex(ord('יִשְׂרָאֵל'[2]))))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First and last letters of בלשנות are respectively: ב and ת\n",
            "This proves us that right-to-left is only a display feature but in memory the characters are in the correct order\n",
            "\n",
            "The word 'בַּלְשָׁנוּת' has a length equal to 12. This is because each nikkud is a combining character to its letter.\n",
            "In the word 'בַּלְשָׁנוּת', we can count 6 nikkudim in addition to the 6 letters of the word giving a count of 12.\n",
            "\n",
            "The second and third characters of 'יִשְׂרָאֵל' are respectively: ִ and ש, with hex values: 0x5b4 and 0x5e9.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J4RQLvApaGT4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data processing\n",
        "\n",
        "We'll be using the data in the attached file *nikkud_seq2seq_data.csv* to train and test our model. This contains Hebrew words without nikkud (vowels), the words with nikkud, and their transliterations (pronunciation written in Latin characters), scraped from articles on the [Hebrew-language Wiktionary](https://he.wiktionary.org/wiki/%D7%A2%D7%9E%D7%95%D7%93_%D7%A8%D7%90%D7%A9%D7%99).\n",
        "\n",
        "**Questions:**\n",
        "  8. Load the data into a Pandas DataFrame variable *df*. How many entries does df contain? Looking at some sample entries, do the transliterations look correct?\n",
        "  9. See if you can find where the transliterations were taken from in Wiktionary. (follow the link above and search for the given words.)"
      ]
    },
    {
      "metadata": {
        "id": "RJ4Wufbtre3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e188907f-ed4f-46a6-eb46-cfc8996fd77d"
      },
      "cell_type": "code",
      "source": [
        "# 8\n",
        "df = pd.read_csv('nikkud_seq2seq_data.csv')\n",
        "df.head()\n",
        "# The df contains 15490 entries. The transliterations look pretty good.\n",
        "\n",
        "# 9\n",
        "# The transliterations are available for each word page, in the 'ניתוח דקדוקי' table, on the 'הגייה' row\n",
        "# An example is eugenika in the following page: https://he.wiktionary.org/wiki/%D7%90%D7%90%D7%95%D7%92%D7%A0%D7%99%D7%A7%D7%94"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>פְּרוֹסְתֵטִית</td>\n",
              "      <td>prostetit</td>\n",
              "      <td>פרוסתטית</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>אֵאוּגֶנִיקָה</td>\n",
              "      <td>eugenika</td>\n",
              "      <td>אאוגניקה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>אֵאוֹזִינוֹפִיל</td>\n",
              "      <td>e'ozinofil</td>\n",
              "      <td>אאוזינופיל</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>אָאוּטִינְג</td>\n",
              "      <td>auting</td>\n",
              "      <td>אאוטינג</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>אָב</td>\n",
              "      <td>av</td>\n",
              "      <td>אב</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            nikkud transliteration        word\n",
              "0   פְּרוֹסְתֵטִית       prostetit    פרוסתטית\n",
              "1    אֵאוּגֶנִיקָה        eugenika    אאוגניקה\n",
              "2  אֵאוֹזִינוֹפִיל      e'ozinofil  אאוזינופיל\n",
              "3      אָאוּטִינְג          auting     אאוטינג\n",
              "4              אָב              av          אב"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "6cl1CB4ct0LK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our model will be simpler if we pad all words to be the same length, and add start- and end-of-word characters. \n",
        "\n",
        "**Questions:**\n",
        "  10. Define variables *nikkud_maxlen* and *translit_maxlen* as the length of the longest word in the *nikkud* and *transliteration* columns, respectively. What are these lengths?\n",
        "  11. Define the function *pad_word* as shown in the comments below, to add start- and end-of-word characters to a word and pad it to a given length."
      ]
    },
    {
      "metadata": {
        "id": "YDngV5AS0JaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "07560c67-3c24-4a33-8441-3fe29a8b50c1"
      },
      "cell_type": "code",
      "source": [
        "nikkud_maxlen = df.nikkud.apply(len).max()\n",
        "translit_maxlen = df.transliteration.apply(len).max()\n",
        "print(\"Longest word in the 'nikkud' and 'transliteration' columns are respectively: {} and {}\".format(nikkud_maxlen, translit_maxlen))\n",
        "\n",
        "def pad_word(word, pad_length):\n",
        "  #### add code here so the function adds ^ to the beginning of the word, spaces  after the word, and $ at the end\n",
        "  #### so that the output string is of length pad_length\n",
        "  #### example: pad_word(\"hello\", 12) should return the string \"^hello     $\" which is of length 12\n",
        "  padding = pad_length-len(word)-2 if pad_length-len(word)-2 > 0 else 0\n",
        "  return '^%s%s$' % (word, ' '*padding)\n",
        "\n",
        "print(\"\\nTesting pad_word('hello', 12) = {}\".format(pad_word('hello', 12)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest word in the 'nikkud' and 'transliteration' columns are respectively: 31 and 25\n",
            "\n",
            "Testing pad_word('hello', 12) = ^hello     $\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AHU7IevT1Bp5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we define strings containing all characters used in our words, along with starting, padding, and ending tokens:"
      ]
    },
    {
      "metadata": {
        "id": "Y_qNYYAbK2ey",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nikkud_charset = '^$ ' + ''.join(sorted(set(''.join(df.nikkud))))\n",
        "translit_charset = '^$ ' + ''.join(sorted(set(''.join(df.transliteration))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8MRAi9H1SsM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "  12. How many characters are used in words with nikkud? In transliterations?\n",
        "  13. Try printing out these character sets? Do you see anything strange in the output? Why?"
      ]
    },
    {
      "metadata": {
        "id": "mbxjzW3I2BQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "42f33f0c-05db-47b7-c1eb-bef17867204a"
      },
      "cell_type": "code",
      "source": [
        "# 12\n",
        "print(\"{} characters are used in the column 'nikkud' and {} in 'transliterations'\".format(len(nikkud_charset), len(translit_charset)))\n",
        "\n",
        "# 13\n",
        "print(\"\\nNikkud character set:\\n{}\".format(nikkud_charset))\n",
        "print(\"\\nTransliteration character set:\\n{}\".format(translit_charset))\n",
        "\n",
        "print(\"\\nThe tav 'ת' character had all the nikkudim merge with it, because these characters are special characters that attach to the preceding letter! 😂\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46 characters are used in the column 'nikkud' and 31 in 'transliterations'\n",
            "\n",
            "Nikkud character set:\n",
            "^$ \"'ְֱֲֳִֵֶַָֹֻּׁׂאבגדהוזחטיךכלםמןנסעףפץצקרשת\n",
            "\n",
            "Transliteration character set:\n",
            "^$ \"'abcdefghijklmnopqrstuvwxyz\n",
            "\n",
            "The tav 'ת' character had all the nikkudim merge with it, because these characters are special characters that attach to the preceding letter! 😂\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5H9VUMPJ1sqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's define functions to produce sequence vectors from words with nikkud or transliterations:"
      ]
    },
    {
      "metadata": {
        "id": "klIXd7952tFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nikkud2sequence(nikkud):\n",
        "  return [nikkud_charset.index(c) for c in pad_word(nikkud, nikkud_maxlen + 2)]\n",
        "def translit2sequence(translit):\n",
        "  return [translit_charset.index(c) for c in pad_word(translit, translit_maxlen + 2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onX7bnt53LJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "  14. What are the feature vectors for \"שָׁלוֹם\" and \"shalom\"? What do the numbers in the vectors mean?\n",
        "  15. Add code to the comment below, to define functions *nikkud2onehot* and *translit2onehot*. These should take in strings (either a Hebrew word with nikkud, or a transliteration) and return a matrix where each character is one-hot encoded. Hint: Use *tf.keras.utils.to_categorical*, with attribute *num_classes = (number of characters in the character set)*.\n",
        "  16. If you implemented those functions correctly, nikkud2onehot('שָׁלוֹם').shape should equal (33, 46) and translit2onehot('shalom').shape should equal (27, 31). What do these dimensions mean?"
      ]
    },
    {
      "metadata": {
        "id": "tM_aLkgn4OE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "00d6bfcc-b044-4282-a25c-4ca9d6f49c71"
      },
      "cell_type": "code",
      "source": [
        "# 14\n",
        "print(\"Vectors for 'שָׁלוֹם' and 'shalom' are respectively:\\n{} \\nand \\n{}\".format(nikkud2sequence('שָׁלוֹם'), translit2sequence('shalom')))\n",
        "      \n",
        "# 15\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def nikkud2onehot(word):\n",
        "  return to_categorical(nikkud2sequence(word), num_classes=46)\n",
        "      \n",
        "def translit2onehot(word):\n",
        "  return to_categorical(translit2sequence(word), num_classes=31)\n",
        "  \n",
        "\n",
        "# 16\n",
        "print(\"\\nThe shapes of nikkud2onehot('שָׁלוֹם') and translit2onehot('shalom') are {} and {}.\".format(nikkud2onehot('שָׁלוֹם').shape, translit2onehot('shalom').shape))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors for 'שָׁלוֹם' and 'shalom' are respectively:\n",
            "[0, 44, 13, 17, 31, 24, 14, 32, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1] \n",
            "and \n",
            "[0, 23, 12, 5, 16, 19, 17, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n",
            "\n",
            "The shapes of nikkud2onehot('שָׁלוֹם') and translit2onehot('shalom') are (33, 46) and (27, 31).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wxq05Qv85osT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's combine the matrixes for all the words together into tensors:"
      ]
    },
    {
      "metadata": {
        "id": "lNnVXLE0LVQ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array([nikkud2onehot(nikkud) for nikkud in df.nikkud])\n",
        "Y = np.array([translit2onehot(translit) for translit in df.transliteration])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jnvo2gdD5xYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that the first dimension of each tensor is the sample size (number of words):"
      ]
    },
    {
      "metadata": {
        "id": "y9iKrVCj5uu8",
        "colab_type": "code",
        "outputId": "33347ed9-0f18-48af-cefe-e07bfed6fdcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X.shape, Y.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15490, 33, 46), (15490, 27, 31))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "qMpe8ST-8BSJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the seq2seq model that we will train, we will try to predict the next character in the transliteration from the characters already generated and from the given nikkud. Since Y contains the encoding for the characters in the transliteration, we want to shift it by one to represent the next character that needs to be predicted.  This is simple with the numpy function *np.roll*. We save this in the tensor Z which will be predicted by the model given X (nikkud) and Y (transliteration):"
      ]
    },
    {
      "metadata": {
        "id": "QoPnnICX7_uE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Z = np.roll(Y, -1, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6myoxPO61FF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3: Seq2seq with LSTMs:"
      ]
    },
    {
      "metadata": {
        "id": "X0y3g2MB9T4a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll now build a seq2seq model with Keras to predict transliteration from nikkud. First let's build and train our model:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "d2DN--WILk47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        },
        "outputId": "5c4c8044-b858-4d35-cdd3-8d28e2d58392"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape = (None, len(nikkud_charset))) ## BONUS\n",
        "encoder = tf.keras.layers.LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape = (None, len(translit_charset))) ## BONUS\n",
        "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences = True, return_state = True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(len(translit_charset), activation = 'softmax') ## BONUS\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
        "model.fit([X, Y], Z, batch_size = 256, epochs = 100, validation_split = 0.2)\n",
        "\n",
        "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
        "                                    initial_state = decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = tf.keras.models.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12392 samples, validate on 3098 samples\n",
            "Epoch 1/100\n",
            "12392/12392 [==============================] - 10s 817us/step - loss: 1.1787 - val_loss: 1.0123\n",
            "Epoch 2/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.9413 - val_loss: 0.8583\n",
            "Epoch 3/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.8734 - val_loss: 0.7867\n",
            "Epoch 4/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.8034 - val_loss: 0.8217\n",
            "Epoch 5/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.7519 - val_loss: 0.7138\n",
            "Epoch 6/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.7140 - val_loss: 0.7248\n",
            "Epoch 7/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.6916 - val_loss: 0.7082\n",
            "Epoch 8/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.6770 - val_loss: 0.6924\n",
            "Epoch 9/100\n",
            "12392/12392 [==============================] - 7s 577us/step - loss: 0.6656 - val_loss: 0.7238\n",
            "Epoch 10/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.6584 - val_loss: 0.7021\n",
            "Epoch 11/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.6518 - val_loss: 0.6466\n",
            "Epoch 12/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.6450 - val_loss: 0.6384\n",
            "Epoch 13/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.6391 - val_loss: 0.6663\n",
            "Epoch 14/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.6358 - val_loss: 0.6457\n",
            "Epoch 15/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.6280 - val_loss: 0.6535\n",
            "Epoch 16/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.6243 - val_loss: 0.6274\n",
            "Epoch 17/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.6088 - val_loss: 0.6239\n",
            "Epoch 18/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.5943 - val_loss: 0.6291\n",
            "Epoch 19/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.5848 - val_loss: 0.5781\n",
            "Epoch 20/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.5709 - val_loss: 0.6861\n",
            "Epoch 21/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.5632 - val_loss: 0.5539\n",
            "Epoch 22/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.5469 - val_loss: 0.5675\n",
            "Epoch 23/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.5334 - val_loss: 0.5929\n",
            "Epoch 24/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.5200 - val_loss: 0.5548\n",
            "Epoch 25/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.5044 - val_loss: 0.5117\n",
            "Epoch 26/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.4913 - val_loss: 0.4790\n",
            "Epoch 27/100\n",
            "12392/12392 [==============================] - 7s 558us/step - loss: 0.4773 - val_loss: 0.4673\n",
            "Epoch 28/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.4636 - val_loss: 0.4545\n",
            "Epoch 29/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.4473 - val_loss: 0.4432\n",
            "Epoch 30/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.4411 - val_loss: 0.4248\n",
            "Epoch 31/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.4268 - val_loss: 0.4178\n",
            "Epoch 32/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.4040 - val_loss: 0.4254\n",
            "Epoch 33/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.3995 - val_loss: 0.5270\n",
            "Epoch 34/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.3869 - val_loss: 0.3948\n",
            "Epoch 35/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.3774 - val_loss: 0.4102\n",
            "Epoch 36/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.3606 - val_loss: 0.3457\n",
            "Epoch 37/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.3440 - val_loss: 0.3397\n",
            "Epoch 38/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.3328 - val_loss: 0.3321\n",
            "Epoch 39/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.3209 - val_loss: 0.3317\n",
            "Epoch 40/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.3130 - val_loss: 0.3177\n",
            "Epoch 41/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.3036 - val_loss: 0.3287\n",
            "Epoch 42/100\n",
            "12392/12392 [==============================] - 7s 560us/step - loss: 0.2916 - val_loss: 0.2955\n",
            "Epoch 43/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.2811 - val_loss: 0.2872\n",
            "Epoch 44/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.2687 - val_loss: 0.2776\n",
            "Epoch 45/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.2553 - val_loss: 0.2645\n",
            "Epoch 46/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.2471 - val_loss: 0.2398\n",
            "Epoch 47/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.2206 - val_loss: 0.4068\n",
            "Epoch 48/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.2112 - val_loss: 0.2350\n",
            "Epoch 49/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.2090 - val_loss: 0.2319\n",
            "Epoch 50/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.1975 - val_loss: 0.2046\n",
            "Epoch 51/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1899 - val_loss: 0.1996\n",
            "Epoch 52/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1861 - val_loss: 0.1858\n",
            "Epoch 53/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1788 - val_loss: 0.1917\n",
            "Epoch 54/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.1640 - val_loss: 0.1674\n",
            "Epoch 55/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1603 - val_loss: 0.1862\n",
            "Epoch 56/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.1613 - val_loss: 0.1509\n",
            "Epoch 57/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1294 - val_loss: 0.1546\n",
            "Epoch 58/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.1366 - val_loss: 0.1406\n",
            "Epoch 59/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1433 - val_loss: 0.1436\n",
            "Epoch 60/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1146 - val_loss: 0.4947\n",
            "Epoch 61/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.1358 - val_loss: 0.3364\n",
            "Epoch 62/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.1173 - val_loss: 0.1659\n",
            "Epoch 63/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1237 - val_loss: 0.1220\n",
            "Epoch 64/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0971 - val_loss: 0.1389\n",
            "Epoch 65/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1187 - val_loss: 0.1395\n",
            "Epoch 66/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.1095 - val_loss: 0.1402\n",
            "Epoch 67/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0990 - val_loss: 0.1170\n",
            "Epoch 68/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.0828 - val_loss: 0.1135\n",
            "Epoch 69/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.1026 - val_loss: 0.1211\n",
            "Epoch 70/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0997 - val_loss: 0.1136\n",
            "Epoch 71/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.0826 - val_loss: 0.2202\n",
            "Epoch 72/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.0863 - val_loss: 0.1088\n",
            "Epoch 73/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0933 - val_loss: 0.1144\n",
            "Epoch 74/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0861 - val_loss: 0.1377\n",
            "Epoch 75/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0644 - val_loss: 0.1103\n",
            "Epoch 76/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0968 - val_loss: 0.1265\n",
            "Epoch 77/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0612 - val_loss: 0.1067\n",
            "Epoch 78/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0769 - val_loss: 0.1232\n",
            "Epoch 79/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0748 - val_loss: 0.1076\n",
            "Epoch 80/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.0734 - val_loss: 0.1156\n",
            "Epoch 81/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0644 - val_loss: 0.3418\n",
            "Epoch 82/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0579 - val_loss: 0.1216\n",
            "Epoch 83/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0712 - val_loss: 0.1014\n",
            "Epoch 84/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.0484 - val_loss: 0.1056\n",
            "Epoch 85/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0690 - val_loss: 0.1137\n",
            "Epoch 86/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0668 - val_loss: 0.1072\n",
            "Epoch 87/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0435 - val_loss: 0.1079\n",
            "Epoch 88/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0687 - val_loss: 0.1012\n",
            "Epoch 89/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0417 - val_loss: 0.1149\n",
            "Epoch 90/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0684 - val_loss: 0.1074\n",
            "Epoch 91/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0636 - val_loss: 0.1536\n",
            "Epoch 92/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0370 - val_loss: 0.1137\n",
            "Epoch 93/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0557 - val_loss: 0.1044\n",
            "Epoch 94/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0557 - val_loss: 0.1004\n",
            "Epoch 95/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0342 - val_loss: 0.1029\n",
            "Epoch 96/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0517 - val_loss: 0.1131\n",
            "Epoch 97/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0564 - val_loss: 0.1720\n",
            "Epoch 98/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0323 - val_loss: 0.1248\n",
            "Epoch 99/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0475 - val_loss: 0.1054\n",
            "Epoch 100/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.0498 - val_loss: 0.1142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r9HjABKj9cu7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on this model, we can decode transliteration from nikkud one character at a time, at each step taking the most likely next character predicted by the model. The function *nikkud2translit* takes in a nikkud string and returns the predicted transliteration:"
      ]
    },
    {
      "metadata": {
        "id": "P-lpIGu5LsvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_text, input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, len(translit_charset))) ## BONUS\n",
        "    target_seq[0, 0, 0] = 1.\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "        char_probabilities = {\n",
        "            c: p for c, p in zip(translit_charset, output_tokens[0, -1, :]) ## BONUS\n",
        "        }\n",
        "        sampled_char = max(translit_charset, key = lambda c: char_probabilities[c]) ## BONUS\n",
        "        sampled_token_index = translit_charset.index(sampled_char) ## BONUS\n",
        "        decoded_sentence += sampled_char\n",
        "        if (sampled_char == '$' or\n",
        "           len(decoded_sentence) > translit_maxlen): ## BONUS\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, len(translit_charset))) ## BONUS\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "def nikkud2translit(nikkud):\n",
        "  tensor = nikkud2onehot(nikkud)[None] ## BONUS\n",
        "  return decode_sequence(nikkud, tensor).replace('$', '').strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EE2GWL95BckG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "  18. Make a new dataframe *df2* containing 100 random samples from *df*. Add a new column *predicted_translit* to the dataframe *df2* with the model's predicted transliteration of the given nikkud. How often does this equal the actual transliteration? What kinds of errors do you see in the output?\n",
        "  17. Change the value of *epochs =* above to train the model on more epochs. How does this affect the loss? How about the observed results?\n",
        "\n",
        "**Bonus:** Modify the problem so that we are instead predicting Hebrew text with nikkud from a transliteration. You will have to switch X and Y, and change code where the comment ## BONUS is written above."
      ]
    },
    {
      "metadata": {
        "id": "Z9vPeAe6Nt7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "1edb4db6-87b2-4a3b-a254-6d5dc70ae456"
      },
      "cell_type": "code",
      "source": [
        "# 18\n",
        "df2 = df.sample(100)\n",
        "df2['predicted_translit'] = df2.nikkud.apply(nikkud2translit)\n",
        "print(\"The model's transliteration is equal to the actual transliteration {}/100 times\\n\".format((df2.transliteration == df2.predicted_translit).sum()))\n",
        "df2.head(20)\n",
        "\n",
        "# The model is pretty much never spot on. Many times though, the first letter of the prediction matches,\n",
        "# which seems to indicate the LSTM model was not trained enough in order to predict well the following letters..."
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model's transliteration is equal to the actual transliteration 0/100 times\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>word</th>\n",
              "      <th>predicted_translit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8424</th>\n",
              "      <td>מַלְטָה</td>\n",
              "      <td>malta</td>\n",
              "      <td>מלטה</td>\n",
              "      <td>ma'ara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10869</th>\n",
              "      <td>עֲבָדִים</td>\n",
              "      <td>'avadim</td>\n",
              "      <td>עבדים</td>\n",
              "      <td>arakha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5659</th>\n",
              "      <td>הַעֹגֶן</td>\n",
              "      <td>ha'ogen</td>\n",
              "      <td>העגן</td>\n",
              "      <td>ma'ara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2994</th>\n",
              "      <td>גַּל</td>\n",
              "      <td>gal</td>\n",
              "      <td>גל</td>\n",
              "      <td>bara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1586</th>\n",
              "      <td>אֶצְבָּעוֹן</td>\n",
              "      <td>etsba'on</td>\n",
              "      <td>אצבעון</td>\n",
              "      <td>arika</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3811</th>\n",
              "      <td>דֶּשֶׁא</td>\n",
              "      <td>deshe</td>\n",
              "      <td>דשא</td>\n",
              "      <td>beret</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11727</th>\n",
              "      <td>פּוֹפְּקוֹרְן</td>\n",
              "      <td>popkorn</td>\n",
              "      <td>פופקורן</td>\n",
              "      <td>kharat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11962</th>\n",
              "      <td>פֶן</td>\n",
              "      <td>fen</td>\n",
              "      <td>פן</td>\n",
              "      <td>khara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>אוֹקוּלְטִיזְם</td>\n",
              "      <td>okultizm</td>\n",
              "      <td>אוקולטיזם</td>\n",
              "      <td>arakha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11850</th>\n",
              "      <td>פִּילִינְג</td>\n",
              "      <td>piling</td>\n",
              "      <td>פילינג</td>\n",
              "      <td>khilut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3564</th>\n",
              "      <td>דִּיסְגְּרָפְיָה</td>\n",
              "      <td>disgrafya</td>\n",
              "      <td>דיסגרפיה</td>\n",
              "      <td>ginit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4859</th>\n",
              "      <td>חֶבֶל</td>\n",
              "      <td>khevel</td>\n",
              "      <td>חבל</td>\n",
              "      <td>kheret</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13805</th>\n",
              "      <td>רֶטְרוֹסְפֶּקְטִיבָה</td>\n",
              "      <td>retrospektiva</td>\n",
              "      <td>רטרוספקטיבה</td>\n",
              "      <td>birut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>אִינֶרְטִי</td>\n",
              "      <td>inerti</td>\n",
              "      <td>אינרטי</td>\n",
              "      <td>inter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13352</th>\n",
              "      <td>קְרִי</td>\n",
              "      <td>kri</td>\n",
              "      <td>קרי</td>\n",
              "      <td>birut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6125</th>\n",
              "      <td>יוֹרֶה</td>\n",
              "      <td>yore</td>\n",
              "      <td>יורה</td>\n",
              "      <td>bara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8926</th>\n",
              "      <td>מַעֲרָכוֹן</td>\n",
              "      <td>markhon</td>\n",
              "      <td>מערכון</td>\n",
              "      <td>ma'ara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14030</th>\n",
              "      <td>שָׁבוּעַ</td>\n",
              "      <td>shavua</td>\n",
              "      <td>שבוע</td>\n",
              "      <td>kharash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3442</th>\n",
              "      <td>צִנוֹרוֹת</td>\n",
              "      <td>tsinorot</td>\n",
              "      <td>צנורות</td>\n",
              "      <td>khilut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12383</th>\n",
              "      <td>בִּסְדוֹם</td>\n",
              "      <td>bisdom</td>\n",
              "      <td>בסדום</td>\n",
              "      <td>birut</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     nikkud transliteration         word predicted_translit\n",
              "8424                מַלְטָה           malta         מלטה             ma'ara\n",
              "10869              עֲבָדִים         'avadim        עבדים             arakha\n",
              "5659                הַעֹגֶן         ha'ogen         העגן             ma'ara\n",
              "2994                   גַּל             gal           גל               bara\n",
              "1586            אֶצְבָּעוֹן        etsba'on       אצבעון              arika\n",
              "3811                דֶּשֶׁא           deshe          דשא              beret\n",
              "11727         פּוֹפְּקוֹרְן         popkorn      פופקורן             kharat\n",
              "11962                   פֶן             fen           פן              khara\n",
              "410          אוֹקוּלְטִיזְם        okultizm    אוקולטיזם             arakha\n",
              "11850            פִּילִינְג          piling       פילינג             khilut\n",
              "3564       דִּיסְגְּרָפְיָה       disgrafya     דיסגרפיה              ginit\n",
              "4859                  חֶבֶל          khevel          חבל             kheret\n",
              "13805  רֶטְרוֹסְפֶּקְטִיבָה   retrospektiva  רטרוספקטיבה              birut\n",
              "760              אִינֶרְטִי          inerti       אינרטי              inter\n",
              "13352                 קְרִי             kri          קרי              birut\n",
              "6125                 יוֹרֶה            yore         יורה               bara\n",
              "8926             מַעֲרָכוֹן         markhon       מערכון             ma'ara\n",
              "14030              שָׁבוּעַ          shavua         שבוע            kharash\n",
              "3442              צִנוֹרוֹת        tsinorot       צנורות             khilut\n",
              "12383             בִּסְדוֹם          bisdom        בסדום              birut"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "tPUAFKNkQR6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "88ea836e-9cb9-4187-8dc2-78271f900557"
      },
      "cell_type": "code",
      "source": [
        "# 19\n",
        "df2 = df.sample(100)\n",
        "df2['predicted_translit'] = df2.nikkud.apply(nikkud2translit)\n",
        "print(\"The new model's transliteration is correct {}/100 times\".format((df2.transliteration == df2.predicted_translit).sum()))\n",
        "df2.head(20)\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The new model's transliteration is correct 90/100 times\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>word</th>\n",
              "      <th>predicted_translit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12613</th>\n",
              "      <td>צַמֶּרֶת</td>\n",
              "      <td>tzameret</td>\n",
              "      <td>צמרת</td>\n",
              "      <td>tsameret</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12898</th>\n",
              "      <td>קוֹמְבִּינָטוֹרִיקָה</td>\n",
              "      <td>kombinatorika</td>\n",
              "      <td>קומבינטוריקה</td>\n",
              "      <td>komyofiya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2926</th>\n",
              "      <td>גִּזָּרוֹן</td>\n",
              "      <td>gizaron</td>\n",
              "      <td>גזרון</td>\n",
              "      <td>gizaron</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11761</th>\n",
              "      <td>פַּח</td>\n",
              "      <td>pakh</td>\n",
              "      <td>פח</td>\n",
              "      <td>pakh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15030</th>\n",
              "      <td>תּוֹרַשְׁתִּי</td>\n",
              "      <td>torashti</td>\n",
              "      <td>תורשתי</td>\n",
              "      <td>torashti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>אַגְנוֹסְטִי</td>\n",
              "      <td>agnosti</td>\n",
              "      <td>אגנוסטי</td>\n",
              "      <td>agnosti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>מְשֻׁשֶּׁה</td>\n",
              "      <td>meshushe</td>\n",
              "      <td>משושה</td>\n",
              "      <td>meshushe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8961</th>\n",
              "      <td>הֲגָאִים</td>\n",
              "      <td>haga'im</td>\n",
              "      <td>הגאים</td>\n",
              "      <td>haga'im</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13626</th>\n",
              "      <td>פָּשׁוּט</td>\n",
              "      <td>pashut</td>\n",
              "      <td>פשוט</td>\n",
              "      <td>pashut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5600</th>\n",
              "      <td>חַשְׁמוֹנַאי</td>\n",
              "      <td>khashmonay</td>\n",
              "      <td>חשמונאי</td>\n",
              "      <td>khashmonay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3508</th>\n",
              "      <td>דֶּטֶרְגֶּנְט</td>\n",
              "      <td>detergnat</td>\n",
              "      <td>דטרגנט</td>\n",
              "      <td>detergnat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9485</th>\n",
              "      <td>מִשְׁמֵשׁ</td>\n",
              "      <td>mishmesh</td>\n",
              "      <td>משמש</td>\n",
              "      <td>mishmesh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2962</th>\n",
              "      <td>חַשְׁמַלִּית</td>\n",
              "      <td>khashmalit</td>\n",
              "      <td>חשמלית</td>\n",
              "      <td>khashmalit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>אֵיקַרְיוֹטִ</td>\n",
              "      <td>ekaryot</td>\n",
              "      <td>איקריוט</td>\n",
              "      <td>ekaryot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8524</th>\n",
              "      <td>מִנּוּי</td>\n",
              "      <td>minuy</td>\n",
              "      <td>מנוי</td>\n",
              "      <td>minuy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13590</th>\n",
              "      <td>רֹאשׁ</td>\n",
              "      <td>rosh</td>\n",
              "      <td>ראש</td>\n",
              "      <td>rosh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>956</th>\n",
              "      <td>אֵלַת</td>\n",
              "      <td>elat</td>\n",
              "      <td>אלת</td>\n",
              "      <td>elat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1644</th>\n",
              "      <td>יַמִּי</td>\n",
              "      <td>yami</td>\n",
              "      <td>ימי</td>\n",
              "      <td>yami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8754</th>\n",
              "      <td>מִסְפָּר</td>\n",
              "      <td>mispar</td>\n",
              "      <td>מספר</td>\n",
              "      <td>mispar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8929</th>\n",
              "      <td>אֶקוֹלוֹגִית</td>\n",
              "      <td>ekologit</td>\n",
              "      <td>אקולוגית</td>\n",
              "      <td>ekologit</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     nikkud transliteration          word predicted_translit\n",
              "12613              צַמֶּרֶת        tzameret          צמרת           tsameret\n",
              "12898  קוֹמְבִּינָטוֹרִיקָה   kombinatorika  קומבינטוריקה          komyofiya\n",
              "2926             גִּזָּרוֹן         gizaron         גזרון            gizaron\n",
              "11761                  פַּח            pakh            פח               pakh\n",
              "15030         תּוֹרַשְׁתִּי        torashti        תורשתי           torashti\n",
              "160            אַגְנוֹסְטִי         agnosti       אגנוסטי            agnosti\n",
              "9411             מְשֻׁשֶּׁה        meshushe         משושה           meshushe\n",
              "8961               הֲגָאִים         haga'im         הגאים            haga'im\n",
              "13626              פָּשׁוּט          pashut          פשוט             pashut\n",
              "5600           חַשְׁמוֹנַאי      khashmonay       חשמונאי         khashmonay\n",
              "3508          דֶּטֶרְגֶּנְט       detergnat        דטרגנט          detergnat\n",
              "9485              מִשְׁמֵשׁ        mishmesh          משמש           mishmesh\n",
              "2962           חַשְׁמַלִּית      khashmalit        חשמלית         khashmalit\n",
              "777            אֵיקַרְיוֹטִ         ekaryot       איקריוט            ekaryot\n",
              "8524                מִנּוּי           minuy          מנוי              minuy\n",
              "13590                 רֹאשׁ            rosh           ראש               rosh\n",
              "956                   אֵלַת            elat           אלת               elat\n",
              "1644                 יַמִּי            yami           ימי               yami\n",
              "8754               מִסְפָּר          mispar          מספר             mispar\n",
              "8929           אֶקוֹלוֹגִית        ekologit      אקולוגית           ekologit"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "kGQSCRx6VuCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "73db9a5f-8508-423f-90cd-eb189a51918d"
      },
      "cell_type": "code",
      "source": [
        "# Extra words not in the training set:\n",
        "print(\"Transliteration of '{}' returned {}\".format('נִסְמַכְתִּי', nikkud2translit('נִסְמַכְתִּי')))\n",
        "print(\"Transliteration of '{}' returned {}\".format('תְהִלָּתִי', nikkud2translit('תְהִלָּתִי')))\n",
        "print(\"Transliteration of '{}' returned {}\".format('לַמְנַצֵּחַ', nikkud2translit('לַמְנַצֵּחַ')))\n",
        "\n",
        "# Althought the results against the training test are pretty impressive,\n",
        "# we can see here that the generalization is not as impressive, altghough not too far"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transliteration of 'נִסְמַכְתִּי' returned nismakhmit\n",
            "Transliteration of 'תְהִלָּתִי' returned tila'it\n",
            "Transliteration of 'לַמְנַצֵּחַ' returned lamatstana\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pFhgEUJmN5AW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        },
        "outputId": "0f8ac078-8095-4342-b08b-7db70dca918c"
      },
      "cell_type": "code",
      "source": [
        "# BONUS QUESTION\n",
        "Z = np.roll(X, -1, axis = 1)\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape = (None, len(translit_charset)))\n",
        "encoder = tf.keras.layers.LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape = (None, len(nikkud_charset)))\n",
        "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences = True, return_state = True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(len(nikkud_charset), activation = 'softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
        "model.fit([Y, X], Z, batch_size = 256, epochs = 100, validation_split = 0.2)\n",
        "\n",
        "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
        "                                    initial_state = decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = tf.keras.models.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_sequence_to_heb(input_text, input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, len(nikkud_charset)))\n",
        "    target_seq[0, 0, 0] = 1.\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "        char_probabilities = {\n",
        "            c: p for c, p in zip(nikkud_charset, output_tokens[0, -1, :])\n",
        "        }\n",
        "        sampled_char = max(nikkud_charset, key = lambda c: char_probabilities[c])\n",
        "        sampled_token_index = nikkud_charset.index(sampled_char)\n",
        "        decoded_sentence += sampled_char\n",
        "        if (sampled_char == '$' or\n",
        "           len(decoded_sentence) > nikkud_maxlen):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, len(nikkud_charset)))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "def translit2nikkud(translit):\n",
        "  tensor = translit2onehot(translit)[None]\n",
        "  return decode_sequence_to_heb(translit, tensor).replace('$', '').strip()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12392 samples, validate on 3098 samples\n",
            "Epoch 1/100\n",
            "12392/12392 [==============================] - 10s 808us/step - loss: 1.3600 - val_loss: 1.1594\n",
            "Epoch 2/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 1.0812 - val_loss: 1.0299\n",
            "Epoch 3/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.9679 - val_loss: 0.9021\n",
            "Epoch 4/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.8501 - val_loss: 0.7959\n",
            "Epoch 5/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.7629 - val_loss: 0.7769\n",
            "Epoch 6/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.7052 - val_loss: 0.7177\n",
            "Epoch 7/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.6723 - val_loss: 0.6699\n",
            "Epoch 8/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.6465 - val_loss: 0.6425\n",
            "Epoch 9/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.6277 - val_loss: 0.6444\n",
            "Epoch 10/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.6150 - val_loss: 0.6091\n",
            "Epoch 11/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.6000 - val_loss: 0.6194\n",
            "Epoch 12/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.5800 - val_loss: 0.6057\n",
            "Epoch 13/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.5641 - val_loss: 0.5696\n",
            "Epoch 14/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.5469 - val_loss: 0.5601\n",
            "Epoch 15/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.5299 - val_loss: 0.5642\n",
            "Epoch 16/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.5156 - val_loss: 0.5291\n",
            "Epoch 17/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.4995 - val_loss: 0.5176\n",
            "Epoch 18/100\n",
            "12392/12392 [==============================] - 7s 576us/step - loss: 0.4815 - val_loss: 0.5364\n",
            "Epoch 19/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.4666 - val_loss: 0.4852\n",
            "Epoch 20/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.4455 - val_loss: 0.4689\n",
            "Epoch 21/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.4254 - val_loss: 0.4527\n",
            "Epoch 22/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.4031 - val_loss: 0.4361\n",
            "Epoch 23/100\n",
            "12392/12392 [==============================] - 7s 576us/step - loss: 0.3833 - val_loss: 0.4201\n",
            "Epoch 24/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.3674 - val_loss: 0.3955\n",
            "Epoch 25/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.3482 - val_loss: 0.3929\n",
            "Epoch 26/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.3324 - val_loss: 0.3740\n",
            "Epoch 27/100\n",
            "12392/12392 [==============================] - 7s 576us/step - loss: 0.3169 - val_loss: 0.3623\n",
            "Epoch 28/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.3021 - val_loss: 0.3999\n",
            "Epoch 29/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.2873 - val_loss: 0.3496\n",
            "Epoch 30/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.2772 - val_loss: 0.3171\n",
            "Epoch 31/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.2616 - val_loss: 0.2964\n",
            "Epoch 32/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.2530 - val_loss: 0.2794\n",
            "Epoch 33/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.2403 - val_loss: 0.2715\n",
            "Epoch 34/100\n",
            "12392/12392 [==============================] - 7s 578us/step - loss: 0.2322 - val_loss: 0.2627\n",
            "Epoch 35/100\n",
            "12392/12392 [==============================] - 7s 576us/step - loss: 0.2208 - val_loss: 0.2654\n",
            "Epoch 36/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.2129 - val_loss: 0.2642\n",
            "Epoch 37/100\n",
            "12392/12392 [==============================] - 7s 577us/step - loss: 0.2043 - val_loss: 0.2520\n",
            "Epoch 38/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1961 - val_loss: 0.2414\n",
            "Epoch 39/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.1871 - val_loss: 0.2400\n",
            "Epoch 40/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1821 - val_loss: 0.2274\n",
            "Epoch 41/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1728 - val_loss: 0.2646\n",
            "Epoch 42/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1670 - val_loss: 0.2198\n",
            "Epoch 43/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1619 - val_loss: 0.2077\n",
            "Epoch 44/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1532 - val_loss: 0.2134\n",
            "Epoch 45/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.1493 - val_loss: 0.2147\n",
            "Epoch 46/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1442 - val_loss: 0.1932\n",
            "Epoch 47/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.1373 - val_loss: 0.2005\n",
            "Epoch 48/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1395 - val_loss: 0.1899\n",
            "Epoch 49/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1309 - val_loss: 0.1715\n",
            "Epoch 50/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1188 - val_loss: 0.1867\n",
            "Epoch 51/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1244 - val_loss: 0.1648\n",
            "Epoch 52/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1268 - val_loss: 0.1666\n",
            "Epoch 53/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.1062 - val_loss: 0.2378\n",
            "Epoch 54/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.1085 - val_loss: 0.1509\n",
            "Epoch 55/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1108 - val_loss: 0.1624\n",
            "Epoch 56/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1053 - val_loss: 0.2036\n",
            "Epoch 57/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.1011 - val_loss: 0.1570\n",
            "Epoch 58/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.0993 - val_loss: 0.1599\n",
            "Epoch 59/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0983 - val_loss: 0.1958\n",
            "Epoch 60/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.0936 - val_loss: 0.1667\n",
            "Epoch 61/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0928 - val_loss: 0.1963\n",
            "Epoch 62/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.0836 - val_loss: 0.1568\n",
            "Epoch 63/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0812 - val_loss: 0.1585\n",
            "Epoch 64/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0827 - val_loss: 0.1951\n",
            "Epoch 65/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0785 - val_loss: 0.1542\n",
            "Epoch 66/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0746 - val_loss: 0.1569\n",
            "Epoch 67/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0798 - val_loss: 0.1727\n",
            "Epoch 68/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0735 - val_loss: 0.1521\n",
            "Epoch 69/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0685 - val_loss: 0.1758\n",
            "Epoch 70/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0686 - val_loss: 0.1539\n",
            "Epoch 71/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0633 - val_loss: 0.1507\n",
            "Epoch 72/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0629 - val_loss: 0.1955\n",
            "Epoch 73/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0682 - val_loss: 0.1512\n",
            "Epoch 74/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0594 - val_loss: 0.1572\n",
            "Epoch 75/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0568 - val_loss: 0.1619\n",
            "Epoch 76/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0529 - val_loss: 0.3876\n",
            "Epoch 77/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0645 - val_loss: 0.2136\n",
            "Epoch 78/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0556 - val_loss: 0.1727\n",
            "Epoch 79/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0521 - val_loss: 0.1601\n",
            "Epoch 80/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0545 - val_loss: 0.1611\n",
            "Epoch 81/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0522 - val_loss: 0.1576\n",
            "Epoch 82/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0479 - val_loss: 0.1647\n",
            "Epoch 83/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0473 - val_loss: 0.1681\n",
            "Epoch 84/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0492 - val_loss: 0.1522\n",
            "Epoch 85/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0458 - val_loss: 0.1701\n",
            "Epoch 86/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0317 - val_loss: 0.1639\n",
            "Epoch 87/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0534 - val_loss: 0.1570\n",
            "Epoch 88/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0419 - val_loss: 0.1672\n",
            "Epoch 89/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0369 - val_loss: 0.1833\n",
            "Epoch 90/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0360 - val_loss: 0.1744\n",
            "Epoch 91/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.0430 - val_loss: 0.1588\n",
            "Epoch 92/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0497 - val_loss: 0.1674\n",
            "Epoch 93/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0396 - val_loss: 0.1631\n",
            "Epoch 94/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.0333 - val_loss: 0.1702\n",
            "Epoch 95/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0354 - val_loss: 0.1695\n",
            "Epoch 96/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0301 - val_loss: 0.1884\n",
            "Epoch 97/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0292 - val_loss: 0.1673\n",
            "Epoch 98/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0359 - val_loss: 0.1707\n",
            "Epoch 99/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0339 - val_loss: 0.1733\n",
            "Epoch 100/100\n",
            "12392/12392 [==============================] - 7s 578us/step - loss: 0.0393 - val_loss: 0.1703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IY8hHi8STyZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "1af91f58-652c-47b9-cade-68e4c5b25f1c"
      },
      "cell_type": "code",
      "source": [
        "# BONUS - TESTING NEW MODEL\n",
        "df2 = df.sample(100)\n",
        "df2['predicted_nikkud'] = df2.transliteration.apply(translit2nikkud)\n",
        "print(\"The transliteration to nikkud model was correct {}/100 times\".format((df2.nikkud == df2.predicted_nikkud).sum()))\n",
        "df2.head(20)\n",
        "\n",
        "# Interestingly, most errors don't fall far away from the expected word.\n",
        "# For example, tsaar was predicted with an Aleph instead of an Ayin and a kamats instead of a Patah.\n",
        "# In this example and many other, the model was able to predict the correct pronunciation.\n",
        "\n",
        "# As these predictions are generated and not based on a dictionary, we could argue \n",
        "# that a prediction that preserves the pronunciation is valid which would boost our accuracy up a lot!"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The transliteration to nikkud model was correct 71/100 times\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>word</th>\n",
              "      <th>predicted_nikkud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12653</th>\n",
              "      <td>צַעַר</td>\n",
              "      <td>tsaar</td>\n",
              "      <td>צער</td>\n",
              "      <td>צַאָר</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4075</th>\n",
              "      <td>הַפָּתוּחַ</td>\n",
              "      <td>hapatu'akh</td>\n",
              "      <td>הפתוח</td>\n",
              "      <td>הַפָּתוּחַ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8466</th>\n",
              "      <td>מַמְזֵר</td>\n",
              "      <td>mamzer</td>\n",
              "      <td>ממזר</td>\n",
              "      <td>מַמְזֵר</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>אֶפְעֶה</td>\n",
              "      <td>ef'e</td>\n",
              "      <td>אפעה</td>\n",
              "      <td>אֶפְעֶה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13432</th>\n",
              "      <td>קָשֶׁה</td>\n",
              "      <td>kashe</td>\n",
              "      <td>קשה</td>\n",
              "      <td>קָשֵׁה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11459</th>\n",
              "      <td>עֵקֶל</td>\n",
              "      <td>ekel</td>\n",
              "      <td>עקל</td>\n",
              "      <td>אֶכֶל</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12799</th>\n",
              "      <td>חֲזִיר</td>\n",
              "      <td>khazir</td>\n",
              "      <td>חזיר</td>\n",
              "      <td>חֲזִיר</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5810</th>\n",
              "      <td>טִלְטוּל</td>\n",
              "      <td>tiltul</td>\n",
              "      <td>טלטול</td>\n",
              "      <td>טִלְטוּל</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4159</th>\n",
              "      <td>הָלַךְ</td>\n",
              "      <td>halakh</td>\n",
              "      <td>הלך</td>\n",
              "      <td>הָלַךְ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>אָלֶלוֹפַּתְיָה</td>\n",
              "      <td>alelopatya</td>\n",
              "      <td>אללופתיה</td>\n",
              "      <td>אָלֶלוֹפַּתְיָה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9478</th>\n",
              "      <td>מִשְׁמַר</td>\n",
              "      <td>mishmar</td>\n",
              "      <td>משמר</td>\n",
              "      <td>מִשְׁמַר</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9832</th>\n",
              "      <td>נַחַל</td>\n",
              "      <td>nakhal</td>\n",
              "      <td>נחל</td>\n",
              "      <td>נַחַל</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>אָמוֹדַאי</td>\n",
              "      <td>amoday</td>\n",
              "      <td>אמודאי</td>\n",
              "      <td>אָמוֹדַאי</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9649</th>\n",
              "      <td>נִבְדָּל</td>\n",
              "      <td>nivdal</td>\n",
              "      <td>נבדל</td>\n",
              "      <td>נִבְדָּל</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10768</th>\n",
              "      <td>סִפּוּן</td>\n",
              "      <td>sipun</td>\n",
              "      <td>ספון</td>\n",
              "      <td>סִפּוּן</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7850</th>\n",
              "      <td>מַזְמֵרָה</td>\n",
              "      <td>mazmera</td>\n",
              "      <td>מזמרה</td>\n",
              "      <td>מַזְמֵרָה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5582</th>\n",
              "      <td>חָשׁוּד</td>\n",
              "      <td>chashud</td>\n",
              "      <td>חשוד</td>\n",
              "      <td>חָשׁוּד</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9456</th>\n",
              "      <td>מִשִּׁכְמוֹ</td>\n",
              "      <td>mishikhmo</td>\n",
              "      <td>משכמו</td>\n",
              "      <td>מִשִּׁכְמוֹ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12631</th>\n",
              "      <td>צִנּוֹר</td>\n",
              "      <td>tsinor</td>\n",
              "      <td>צנור</td>\n",
              "      <td>צִנּוֹר</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>טוֹב</td>\n",
              "      <td>tov</td>\n",
              "      <td>טוב</td>\n",
              "      <td>טוֹב</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                nikkud transliteration      word predicted_nikkud\n",
              "12653            צַעַר           tsaar       צער            צַאָר\n",
              "4075        הַפָּתוּחַ      hapatu'akh     הפתוח       הַפָּתוּחַ\n",
              "8466           מַמְזֵר          mamzer      ממזר          מַמְזֵר\n",
              "1548           אֶפְעֶה            ef'e      אפעה          אֶפְעֶה\n",
              "13432           קָשֶׁה           kashe       קשה           קָשֵׁה\n",
              "11459            עֵקֶל            ekel       עקל            אֶכֶל\n",
              "12799           חֲזִיר          khazir      חזיר           חֲזִיר\n",
              "5810          טִלְטוּל          tiltul     טלטול         טִלְטוּל\n",
              "4159            הָלַךְ          halakh       הלך           הָלַךְ\n",
              "906    אָלֶלוֹפַּתְיָה      alelopatya  אללופתיה  אָלֶלוֹפַּתְיָה\n",
              "9478          מִשְׁמַר         mishmar      משמר         מִשְׁמַר\n",
              "9832             נַחַל          nakhal       נחל            נַחַל\n",
              "990          אָמוֹדַאי          amoday    אמודאי        אָמוֹדַאי\n",
              "9649          נִבְדָּל          nivdal      נבדל         נִבְדָּל\n",
              "10768          סִפּוּן           sipun      ספון          סִפּוּן\n",
              "7850         מַזְמֵרָה         mazmera     מזמרה        מַזְמֵרָה\n",
              "5582           חָשׁוּד         chashud      חשוד          חָשׁוּד\n",
              "9456       מִשִּׁכְמוֹ       mishikhmo     משכמו      מִשִּׁכְמוֹ\n",
              "12631          צִנּוֹר          tsinor      צנור          צִנּוֹר\n",
              "7237              טוֹב             tov       טוב             טוֹב"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "wRI7VUw4XP0t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c35a8096-817b-4966-a456-17f064a0a36c"
      },
      "cell_type": "code",
      "source": [
        "# Extra words not in the training set:\n",
        "print(\"Transliteration of '{}' returned {}\".format('trumpeldor', translit2nikkud('trumpeldor')))\n",
        "print(\"Transliteration of '{}' returned {}\".format('jeremy', translit2nikkud('jeremy')))\n",
        "print(\"Transliteration of '{}' returned {}\".format(\"raanana\", translit2nikkud(\"raanana\")))\n",
        "print(\"Transliteration of '{}' returned {}\".format(\"titkhadesh\", translit2nikkud(\"titkhadesh\")))\n",
        "\n",
        "# The performance on these words is interesting, but not as impressive\n",
        "# We can conclude that our model is very likely overfitting "
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transliteration of 'trumpeldor' returned טְרוּמְדֶּנְטֶרְי\n",
            "Transliteration of 'jeremy' returned גֶ'רֶמֶה\n",
            "Transliteration of 'raanana' returned אֲרָנָלוֹ\n",
            "Transliteration of 'titkhadesh' returned טִיְחַדְךְ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}